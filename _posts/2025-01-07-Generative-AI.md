---
layout: post
title: Generative AI
date: 2025-01-07 14:00:00-0400
---

A generative model is a *joint* probability distribution $p(x)$, for $x\in\mathcal{X}$ . It's a joint distribution because $x$ can be multidimensional where, i.e. consists of multiple variablesÂ  $(x_1, x_2, \ldots, x_n)$. 

We also have conditional generative model $p(x\vert c)$ in which the generative model would be conditioned on inputs or covariates $c\in C$.


## Types of generative Models

*  **Probabilistic graphical models (PGM):** uses simple, often linear, mappings to map a set of interconnected latent variables $z_1, \ldots, z_L$ to observed variables $x_1, \ldots, x_D$. 
* ***Deep Generative Models (DGM):** uses deep neural networks to learn a complex mapping from a single latent vector $z$ to the observed data $x$. Types of DGM are
	* **Variational Autoencoders (VAE)**
	* **AutoRegressive Models (ARM) models**
	* **Normalizing Flows**
	* **Diffusion Models** 
	* **Energy Based Models (EBM)**
	* **Generative Adversarial Networks (GAN)**

The following table summarizes the different generative models across different aspects (we'll discuss why in the next chapters):

| **Model**                                                     | **Density**                                                                                                                                         | **Sampling**                                                                                                                   | **Training**                                                                                                                | **Latents**                                                                                                                      | **Architecture**                                                                                                                   |
| ------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- |
| **PGM-D**<br>i.e., Probabilistic Graphical Model - Directed   | Exact, fast: The joint distribution $p(x)$ can be computed exactly and efficiently, leveraging the directed graph structure.                        | Fast: Sampling is efficient using ancestral sampling, which sequentially samples variables based on dependencies in the DAG.   | MLE: Trained using Maximum Likelihood Estimation, which directly optimizes the likelihood of observed data.                 | Optional: Latent variables (hidden variables) can be included but are not required for the model.                                | Sparse DAG: The model uses a sparse Directed Acyclic Graph, where edges capture directed dependencies.                             |
| **PGM-U**<br>i.e., Probabilistic Graphical Model - Undirected | Approximate, slow: The joint distribution $p(x)$ requires approximations due to the intractable partition function, making it computationally slow  | Slow: Sampling typically involves computationally expensive methods like Markov Chain Monte Carlo (MCMC).                      | MLE-A: Trained using approximate Maximum Likelihood Estimation, as exact computation of likelihood is infeasible.           | Optional: Latent variables can be included but are not mandatory for the model.                                                  | Sparse graph: The model uses a sparse undirected graph, where edges represent mutual dependencies.                                 |
| **VAE**<br>i.e., Variational Auto Encoder                     | LB, fast: Provides a **lower bound** on the likelihood (e.g., Evidence Lower Bound, or ELBO) and is computationally efficient for density modeling. | Fast: Efficient sampling is achieved via reparameterization, enabling smooth gradient-based optimization in latent space.      | MLE-LB: Trained by maximizing a lower bound on the likelihood, balancing reconstruction and latent regularization.          | $\mathbb{R}^L$: Latent representations (e.g., a compressed representation of the data) are a central feature of the model.       | Encoder-Decoder: Uses an encoder to map data to latent space and a decoder to reconstruct the data from the latent space.          |
| **ARM**<br>i.d., AutoRegressive Model                         | Exact, fast: The joint distribution $p(x)$ is computed exactly and efficiently using the sequential nature of the model.                            | Slow: Sampling is sequential, requiring one variable to be sampled at a time, which increases computation time.                | MLE: Trained using Maximum Likelihood Estimation, directly optimizing the likelihood of sequentially modeled variables.     | None: Does not use latent variables; it explicitly models the observed data.                                                     | Sequential: Processes data one variable at a time, reflecting the sequential dependency structure of the model.                    |
| **Flows**                                                     | Exact, slow/fast: Exact computation of $p(x)$, but speed depends on the specific normalizing flow architecture and its invertible transformations.  | Slow: Sampling involves applying invertible transformations, which can be computationally expensive depending on the model.    | MLE: Trained using Maximum Likelihood Estimation by directly optimizing the likelihood of the transformed data.             | $\mathbb{R}^D$: Latent representations are central, as flows map data to and from latent space using invertible transformations. | Invertible: Uses invertible transformations to map between data and latent space, ensuring exact density computation.              |
| **EBM**, <br>i.e., Energy Based Models                        | Approx, slow: Density estimation is approximate due to the need to compute a complex energy-based objective, which is computationally expensive.    | Slow: Sampling often relies on expensive iterative methods like Langevin Dynamics to generate samples.                         | MLE-A: Trained using approximate Maximum Likelihood Estimation due to challenges in normalizing the energy function.        | Optional: Latent variables can be included but are not essential for energy-based models.                                        | Discriminative: Models the energy function to differentiate between observed and unobserved data rather than direct probabilities. |
| **Diffusion**                                                 | LB: Provides a **lower bound** on likelihood during training by modeling a sequence of forward and reverse processes.                               | Slow: Sampling involves iterative denoising steps (e.g., reversing the diffusion process), which is computationally intensive. | MLE-LB: Trained by maximizing a lower bound on the likelihood, optimizing the reconstruction of data from corrupted inputs. | $\mathbb{R}^D$: Latent representations are central, as the diffusion process maps data into progressively noisier latent spaces. | Encoder-Decoder: Uses an encoder to add noise to data (diffusion) and a decoder to reverse the process (denoising).                |
| **GAN**, <br>i.e., Generative Adversarial Networks            | NA: Does not explicitly model the density $p(x)$; instead, it learns to generate data by adversarial training.                                      | Fast: Sampling is efficient, as the generator directly maps random noise to generated data in one forward pass.                | Min-max: Trained using adversarial training, where a generator and discriminator compete to improve data generation.        | $\mathbb{R}^L$: Latent representations (e.g., random noise vectors) are central to generating data.                              | Generator-Discriminator: Combines a generator (to create data) and a discriminator (to evaluate its realism).                      |


## Goals of Generative AI

1. **Data Generation**: One of the primary goals of generative AI is **data generation**, where models create new data samples that resemble the original data they were trained on. This includes generating realistic images, text, audio, or other forms of data. Generative AI is also used for various tasks, such as:
	- **Creating synthetic data** for training discriminative models.
	- **Conditional generation** to control outputs based on specific inputs, enabling applications such as:
	    - Text-to-image (e.g., generating an image from a text description).
	    - Image-to-text (e.g., image captioning).
	    - Image-to-image (e.g., colorization, inpainting, uncropping, and restoration).
	    - Speech-to-text (e.g., automatic speech recognition).
	    - Sequence-to-sequence (e.g., machine translation or text continuation).

The difference between conditional generative models and discriminative models lies in the outputs: generative models allow multiple valid outputs, whereas discriminative models assume a single correct output.