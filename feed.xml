<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="https://azareei.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://azareei.github.io/" rel="alternate" type="text/html" /><updated>2023-03-27T17:15:02+10:00</updated><id>https://azareei.github.io/feed.xml</id><title type="html">Ahmad (Ady) Zareei</title><subtitle>Personal homepage of Ahmad Zareei.
</subtitle><entry><title type="html">Summary of Investments by Bodie</title><link href="https://azareei.github.io/blog/2022/Bodie-Investments/" rel="alternate" type="text/html" title="Summary of Investments by Bodie" /><published>2022-11-11T08:30:00+10:00</published><updated>2022-11-11T08:30:00+10:00</updated><id>https://azareei.github.io/blog/2022/Bodie-Investments</id><content type="html" xml:base="https://azareei.github.io/blog/2022/Bodie-Investments/"><![CDATA[<p>This is a sample test</p>]]></content><author><name></name></author><summary type="html"><![CDATA[investment]]></summary></entry><entry><title type="html">Bayesian Machine Learning</title><link href="https://azareei.github.io/blog/2022/Baysian-ml/" rel="alternate" type="text/html" title="Bayesian Machine Learning" /><published>2022-01-26T08:30:00+10:00</published><updated>2022-01-26T08:30:00+10:00</updated><id>https://azareei.github.io/blog/2022/Baysian-ml</id><content type="html" xml:base="https://azareei.github.io/blog/2022/Baysian-ml/"><![CDATA[<p>These is a review of Bayesian Machine Learning. It’s based on HSE’s course on this topic.</p>

<h3 id="maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</h3>

<p>The ML estimate for \(\theta\) is the value under which the data are most likely, i.e.</p>

\[\boxed{\hat{\theta}_{ML} \in \text{argmax}_\theta p(y|\theta)}\]

<p>Example: We know that a fair coin has \(p(\text{head}) = p(\text{tail}) = 1/2\). Now imagine you have a coin and you don’t know if it is fair or not. You decide to throw the coin \(m\) times, where you observe \(m_H\) times head, and \(m_T\) times tail, where \(m_H + m_T = m\). Now you ask given this observation, what is the probability of \(p(\text{head}), p(\text{tail})\) for this coin? We know that if \(p(\text{head}) =\theta\), then the likelihood of such observation is</p>

\[p(y|\theta) = \theta^{m_H} (1-\theta)^{m-m_H}\]

<p>Now, we are interested to find a \(\hat \theta_{ML}\) such that the above probability is maximized. Instead of the probability, we maximize its logarithm which will be easier. We have</p>

\[l(\theta) = m_H \log(\theta) + (m-m_H) \log(1-\theta)\]

<p>finding the \(\text{argmax}_\theta\) for the above equation we have</p>

\[\frac{d}{d\theta}d(\theta)|_{\hat\theta} = 0 = \frac{m_H}{\theta}|_{\hat\theta} - \frac{m-m_H}{1-\theta}|_{\hat\theta}\]

\[\longrightarrow \hat{\theta} = \frac{m_H}{m}\]

<h3 id="maximum-a-posteriori-map-estimation">Maximum a posteriori (MAP) estimation</h3>

<p>We learned that in MLE estimation we find \(\theta\) to maximize the likelihood function \(p(y\mid \theta)\). Now, in MAP, we find \(\theta\) such that the posterior \(p(\theta\mid y)\) is maximized, i.e.,</p>

\[\boxed{\hat{\theta}_{MAP} \in \text{argmax}_\theta p(\theta\mid y) = \text{argmax}_\theta  \frac{p(y\mid \theta) p(\theta)}{p(y)}}\]

<p>Since the denominator does not depend on \(\theta\), we have</p>

\[\hat{\theta}_{MAP} = \text{argmax}_\theta p(y\mid \theta)p(\theta) = \text{argmax}_\theta \log p(y\mid \theta) + \log p(\theta)\]

<p>The last term, \(\log p(\theta)\) is known as our prior belief.</p>

<p>Example: Again, imagine the same coin flip problem. Imagine we take the prior to be a beta distribution as</p>

\[p(\theta) = \beta(\theta;\alpha,\beta) = A~ \theta^\alpha (1-\theta)^{\beta}\]

<p>where \(A\) is a constant to make \(\int p(\theta)\text{d}\theta=1\). Now finding the solution to the MAP estimator, we find that</p>

\[\hat{\theta}_{MAP} = \text{argmax}_\theta \log p(y\mid \theta) + \log p(\theta)\]

\[\frac{m_H}{\theta}\mid _{\hat\theta} - \frac{m-m_H}{1-\theta}\mid _{\hat\theta}  + \frac{\alpha}{\theta}\mid _{\hat\theta} - \frac{\beta}{1-\theta}\mid _{\hat\theta} = 0\]

\[\hat{\theta}_{MAP} = \frac{m_H + \alpha}{m+\alpha+\beta }\]

<p>The point of \(\alpha, \beta\) coming from the prior in the above equation will be a regularizer. Imagine, we only throw the coin only once, then the ML gives \(\hat{\theta}_{ML}=1\), however, \(\hat{\theta}_{MAP} = \alpha/(\alpha+\beta)\).</p>

<h3 id="point-estimation-and-probabilistic-linear-regression">Point estimation and probabilistic linear regression</h3>

<p>Imagine we are given $m$ datapoints, \((x_1,y_1), \cdots, (x_m,y_m)\), where \(x_i\)’s are independent variables and \(y_i\)’s are dependent. Assuming a linear relationship as</p>

\[y_i \sim \theta^\top x_i + \epsilon\]

<p>where \(\epsilon \sim \mathcal{N}(0,\sigma^2)\). Assuming the independence between the \(y_i\)’s, we can write</p>

\[p(y\mid x,\theta)= \Pi_{i=1}^m p(y_i\mid x_i,\theta) = \Pi_{i=1}^m \mathcal{N}(y_i;\theta^\top x_i,\sigma^2)\]

<p>Finding the maximum of log-likelihood, we obtain</p>

\[\hat{\theta}_{ML} = \text{argmax}_\theta p(y\mid x,\theta) = \text{argmin}_\theta \sum_{i=1}^m (y_i - \theta^\top x_i)^2\]

<p>which is the equivalent to ordinary least squares.</p>

<p>Now, trying the MAP estimate, we first need to assume a prior for the \(\theta\) variables. As an initial estimate we assume \(\theta \sim \mathcal{N}(0, \nu^2\mathbf{I})\). Finding the MAP solution we find that</p>

\[\hat{\theta}_{MAP} = \text{argmax}_\theta \log p(y\mid \theta) + \log p(\theta)\]

\[\hat{\theta}_{MAP} = \text{argmin}_\theta \sum_{i=1}^m (y_i-\theta^\top x_i)^2 + \frac{\sigma^2}{\nu^2} \mid \mid \theta\mid \mid ^2\]

<p>which is the same as ML solution with a regularization term.</p>

<h3 id="conjugate-priors">Conjugate priors</h3>

<p>We often would like to find the full posterior, \(p(\theta\mid y)\). If we know the full posterior distribution, then we can do posterior predictive distribution as</p>

\[p(y_{m+1}\mid y) = \int p(y_{m+1}\mid y,\theta)~p(\theta\mid y) d\theta = \int p(y_{m+1}\mid \theta)~p(\theta\mid y) d\theta\]

<p>The second part is because \(y_{m+1}\) and \(y\) are conditionally independent. If we would like to find the full posterior, then</p>

\[p(\theta\mid y) = \frac{p(y\mid \theta)p(\theta)}{p(y)}\]

<p>The bottom part is just a normalizer. So we need to choose the prior, \(p(\theta)\), such that with the likelihood distribution, \(p(y\mid \theta)\), we are able to track the distribution. Selecting prior in such a way that the \(p(y\mid \theta)p(\theta)\) becomes tractable is known as a selection of conjugate priors for \(p(y\mid \theta)\).</p>

<h3 id="exponential-families-and-conjugate-priors">Exponential families and conjugate priors</h3>

<p>The family of distribution \(\mathcal{F}\) is called exponential family if every member of \(\mathcal{F}\) has the form</p>

\[\boxed{p(y_i\mid \theta) = f(y) g(\theta)\exp(\phi(\theta)^\top u(y_i))}\]

<p>where $f(\cdot), g(\cdot), \phi(\cdot),u(\cdot)$ are some functions. For example, an exponential distribution is an exponential family distribution:</p>

\[p(y\mid \theta) = \theta e^{-\theta y}\]

<p>or a beta distribution:</p>

\[p(y\mid \theta) = \theta^{y} (1-\theta)^{1-y} = \exp(y\log\theta + (1-y)\log(1-\theta)) = (1-\theta)\log(y \log\frac{\theta}{1-\theta})\]

<p>Normal distribution is also an exponential family.</p>

<p>If the \(p(y_i\mid \theta)\) is coming from an exponential family distribution, then if \(y_i\)’s are independent, the likelihood becomes</p>

\[p(y\mid \theta) = \Pi_{i=1}^m p(y_i\mid \theta) = \left[\Pi_{i=1}^m f(y_i)\right] g(\theta)^m \exp\left( \phi(\theta)^\top \sum_{i=1}^m u(y_i)\right)\]

<p>Now, we can select the prior as</p>

\[p(\theta) \sim g(\theta)^\eta \exp(\phi(\theta)^\top \nu)\]

<p>where \(\eta,\nu\) are hyper-parameters.</p>

<p>Now you might ask, doesn’t the choice of prior doesn’t matter? Can we really pick any prior distribution? <strong>In the next part, we show that as long as you pick a prior that assigns non-zero probabilities to every possible value of $\theta$, the solution of MAP converges to the true solution $\theta^<em>$, or more precisely, $\theta^</em>$ corresponds to the likelihood model which is closest to the true generating distribution. The closeness of the distributions is defined using KL divergence.</strong></p>

<h3 id="kl-divergence">KL divergence</h3>

<p>The KL divergence between two distributions \(p\) and \(q\) is defined as</p>

\[\boxed{D_{KL}(p\mid \mid q) := \int_x p(x) \log \frac{p(x)}{q(x)}dx}\\ \to D_{KL}(p\mid \mid q)= \mathbf{E}_p[\log p(x) - \log q(x)]\]

<p>Note that \(D_{KL}(p\mid \mid q)\) is always positive since</p>

\[-D_{KL}(p\mid \mid q) = \int p(x)\left(-\log \frac{p(x)}{q(x)}\right) dx \leq  -\log \int p(x) \frac{q(x)}{p(x)} dx =0\]

\[\rightarrow D_{KL}(p\mid \mid q)\geq 0\]

<p>and the equality with zero only happens when \(p=q\). Now, we want to use the KL divergence to find the distribution from the likelihood family that is closest to the true generating function</p>

\[\theta^* = \text{argmin}_\theta D(q(\cdot) \mid \mid p(\cdot\mid \theta))\]

<p>Imagine we have \(y=(y_1,\cdots, y_m)\) as a set of independent samples from an arbitrary distribution \(q(y)\). We assume a family \(\mathcal{F}\) of likelihood distributions with \(\Theta = \{ \theta: p(\cdot \mid  \theta) \in \mathcal{F}\}\) the finite parameter space. Our goal is to show that if \(p(\theta=\theta^*)&gt;0\), then \(p(\theta=\theta^*\mid y)\to1\) as the number of observation increases or \(m\to\infty\). Consider \(\theta\neq \theta^*\), then</p>

\[\log \frac{p(\theta\mid y)}{p(\theta^*\mid y)} = \log \frac{p(y\mid \theta) p (\theta)}{p(y\mid \theta^*) p (\theta^*)} = \log \frac{p (\theta)}{p (\theta^*)} + \sum_{i=1}^m \log \frac{p(y_i\mid \theta) }{p(y_i\mid \theta^*)}\]

<p>where we used the fact that \(p(\theta^*) \neq 0\). We have</p>

\[\frac{1}{m} \sum_{i=1}^m \log \frac{p(y_i\mid \theta) }{p(y_i\mid \theta^*)} \to \mathbf{E}_q\left[ \log \frac{p(y_i\mid \theta) }{p(y_i\mid \theta^*)}\right]\]

<p>Expanding the Expected value we have</p>

\[\mathbf{E}_q\left[ \log \frac{p(y_i\mid \theta) }{p(y_i\mid \theta^*)}\right] = \mathbf{E}_q\left[ \log \frac{p(y_i\mid \theta) }{p(y_i\mid \theta^*)}\frac{q(y)}{q(y)}\right] = \mathbf{E}_q\left[ \log \frac{q(y) }{p(y_i\mid \theta^*)}- \log \frac{q(y)}{p(y_i\mid \theta)} \right]\]

\[\mathbf{E}_q\left[ \log \frac{p(y_i\mid \theta) }{p(y_i\mid \theta^*)}\right] = D_{KL}\left(q(\cdot) \mid \mid  p(\cdot\mid \theta^*)\right) - D_{KL}\left(q(\cdot) \mid \mid  p(\cdot\mid \theta)\right) &lt; 0\]

<p>The above result is negative since we assume that \(\theta^*\) minimizes the KL divergence between \(q(\cdot)\) and \(p(\cdot\mid \theta)\). So far we have found that</p>

\[\sum_{i=1}^m \log \frac{p(y_i\mid \theta) }{p(y_i\mid \theta^)} \to m\mathbf{E}_q\left[ \log \frac{p(y_i\mid \theta) }{p(y_i\mid \theta^)}\right] \to -\infty\]

<p>since the expected value we found to be negative, and as \(m\to\infty\), the value goes to negative infinity. Plugging back into the initial equation we have</p>

\[\log \frac{p(\theta\mid y)}{p(\theta^*\mid y)} = \log \frac{p (\theta)}{p (\theta^*)} + \sum_{i=1}^m \log \frac{p(y_i\mid \theta) }{p(y_i\mid \theta^*)} = \log \frac{p (\theta)}{p (\theta^*)} -\infty \to -\infty\]

\[\log \frac{p(\theta\mid y)}{p(\theta^*\mid y)}  \to -\infty\]

<p>This implies that \(p(\theta\mid y)/p(\theta^*\mid y) \to 0\), which means that \(p(\theta\mid y) \to 0\). We started with the fact that \(\theta\neq\theta^*\). So if \(p(\theta\mid y)\to 0\) for \(\theta\neq\theta^*\), then \(p(\theta^*\mid y)\to 1\).</p>

<h1 id="expectation-maximization">Expectation Maximization</h1>

<h3 id="guassian-mixture-models">Guassian Mixture Models:</h3>

<p>In Gaussian Mixture model, you assume that your data is coming from a combination of Gaussian (Gaussian Mixture). There is a latent variable, \(z\), that determines which Gaussian to pick or how to combine the Gaussian models. In this latent model, we are interested to find \(\theta\) parameters of the Gaussian, such that \(p(x\mid \theta)\) is maximized. Let’s assume that the latent variable is discrete and \(z=1\) or \(2\). The probability of observing a datapoint is</p>

\[p(x\mid \theta) = \sum_{c=1}^2 p(x,z=c\mid \theta) = \sum_{c=1}^2 p(z=c)~p(x\mid \theta,z)\]

<p>Our goal as usual is to find \(\max_\theta p(X\mid \theta))\). We have</p>

\[\max_\theta p(X\mid \theta) = \max_\theta \log p(X\mid \theta)= \max_\theta \log \Pi_{i=1}^N p(x_i\mid \theta)=  \max_\theta \sum_{i=1}^N \log p(x_i\mid \theta)\]

\[\max*\theta p(X\mid \theta) = \max*\theta \sum*{i=1}^N \log \sum*{c=1}^2 p(x*i,z_i=c\mid \theta)\\ = \max*\theta \sum*{i=1}^N \log \sum*{c=1}^2 \frac{q(z*i=c)}{q(z_i=c)}p(x_i,z_i=c\mid \theta) \\ = \max*\theta \sum*{i=1}^N \log \sum*{c=1}^2q(z*i=c) \frac{p(x_i,z_i=c\mid \theta)}{q(z_i=c)}) \\ \geq \max*\theta \sum*{i=1}^N \sum*{c=1}^2 q(z_i=c) \log \frac{p(x_i,z_i=c\mid \theta)}{q(z_i=c)})\]

<p>So, in summary we have found the following</p>

\[\boxed{\log p(X\mid \theta) \geq \mathcal{L}(\theta,q) \text{ for any } q }\\ \boxed{\mathcal{L}(\theta,q) = \sum_{i=1}^N \sum_{c=1}^2 q(z_i=c) \log   \frac{p(x_i,z_i=c\mid \theta)}{q(z_i=c)}) }\]

<p>So, we do the above part in two steps as follows (Expectation step)</p>

\[\boxed{q^{k+1} = \text{argmax}_q \mathcal{L}(\theta^k,q)}\]

<p>and next (Maximization step)</p>

\[\boxed{\theta^{k+1} = \text{argmax}_\theta \mathcal{L}(\theta,q^{k+1})}\]

<h3 id="e-step">E-Step:</h3>

<p>Let’s look at the E-Step. In order to do so, let’s look at the difference between the log-likelihood and the lowerbound that we defined</p>

\[\log p(X,\theta)-\mathcal{L}(\theta,q) = \sum_{i=1}^N \log p(x_i\mid \theta)  - \sum_{i=1}^N \sum_{c=1}^2 q(z_i=c) \log   \frac{p(x_i,z_i=c\mid \theta)}{q(z_i=c)}) \\ = \sum_{i=1}^N \log p(x_i\mid \theta) \sum_{c=1}^2 q(z_i=c)  - \sum_{i=1}^N \sum_{c=1}^2 q(z_i=c) \log   \frac{p(x_i,z_i=c\mid \theta)}{q(z_i=c)}) \\ = \sum_{i=1}^N \sum_{c=1}^2 q(z_i=c)  \left( \log p(x_i\mid \theta)  -\log   \frac{p(x_i,z_i=c\mid \theta)}{q(z_i=c)}) \right)\\ = \sum_{i=1}^N \sum_{c=1}^2 q(z_i=c)  \left( \log   \frac{q(z_i=c) p(x_i\mid \theta) }{p(x_i,z_i=c\mid \theta)}) \right) \\ = \sum_{i=1}^N \sum_{c=1}^2 q(z_i=c)  \left( \log   \frac{q(z_i=c) }{p(z_i=c\mid x_i,\theta)}) \right) \\ = D_{KL}\left({q(z_i=c) }\mid \mid {p(z_i=c\mid x_i,\theta)} \right)\]

<p>So we found that</p>

\[\boxed{\log p(X\mid \theta) - \mathcal{L}(\theta,q) = D_{KL}\left({q(z_i=c) }\mid \mid {p(z_i=c\mid x_i,\theta)} \right)}\]

<p>which basically means that to maximize lowerbound \(\mathcal{L}(\theta,q)\) (which minimizes the distance between the log-likelihood and the lowerbound), we need to minimize the KL-divergence on the righthandside. The KL divergence is zero when the two PDFs are the same, so</p>

\[q(z_i=c) = p(z_i=c\mid x_i,\theta)\]

<h3 id="m-step">M-Step:</h3>

<p>Now we are interested to maximize the following with respect to \(\theta\) as</p>

\[\mathcal{L}(\theta,q) = \sum_{i=1}^N \sum_{c=1}^2 q(z_i=c) \log   \frac{p(x_i,z_i=c\mid \theta)}{q(z_i=c)})  \\= \sum_{i=1}^N \sum_{c=1}^2 q(z_i=c) \log   {p(x_i,z_i=c\mid \theta)}) - \sum_{i=1}^N \sum_{c=1}^2 q(z_i=c) \log   {q(z_i=c)})\\ = \mathbf{E}_q \log p(X,Z\mid \theta)\]

<h3 id="summary-of-expectation-maximization">Summary of Expectation-Maximization:</h3>

<p>E-step:</p>

\[\boxed{q^{k+1}(z_i) = p(z_i\mid x_i,\theta^k) }\]

<p>which results in the fact that \(\log p(X,\theta) = \mathcal{L}(\theta,q^{k+1})\). Next for the M-step, we have</p>

\[\boxed{\theta^{k+1} = \text{argmax}_\theta \mathbf{E}_q \log p(X,Z\mid \theta)}\]

<p>Note that this maximizes the lower bound, however it is guaranteed that \(\log p(X\mid \theta^{k+1}) \geq \mathcal{L}(\theta^{k+1},q^{k+1})\)</p>

<h3 id="convergence">Convergence</h3>

<p>We have</p>

\[\log p(X\mid \theta^{k+1}) \geq \mathcal{L}(\theta^{k+1},q^{k+1}) \geq \mathcal{L}(\theta^k,q^{k+1}) = \log p(X\mid \theta^k)\]

<h3 id="gaussian-mixture-models-gmm">Gaussian Mixture Models (GMM)</h3>

<p>In these models, we assume that the data is coming from a mixture of Gaussian distributions \(\mathcal{N}(\mu,\Sigma)\) and the latent distribution is a categorical distribution \(\phi\). This basically means that</p>

\[\boxed{p(X) = \sum_{k=1}^K \pi_k \mathcal{N}(X\mid \mu_k,\Sigma_k)}\]

<p>where \(\sum_k \pi_k = 1\). Note that \(\theta = \{ \mu_1,\Sigma_1, \cdots, \mu_K,\Sigma_K\}.\) Now, let’s assume the latent variable is called \(z_i\). Using Expectation-Maximization that we discussed here, we find that</p>

\[q(z_i = k) = p(z_i=k\mid x_i,\theta) = \frac{p(z_i=k) p(x_i \mid  z_i=k,\theta) }{p(x_i\mid \theta)} = \frac{p(z_i=k)p(x_i \mid  z_i=k,\theta) }{\sum_{k} p(z_i=k) p(x_i \mid  z_i=k,\theta) } \\ \boxed{q(z_i=k) = \frac{\pi_k \mathcal{N}(x_i\mid \mu_k,\Sigma_k)}{\sum_k\pi_k \mathcal{N}(x_i\mid \mu_k,\Sigma_k)}}\]

<p>Next, in the Maximization step, we have</p>

\[\theta^*= \max_\theta \mathbf{E}_q \log p(X,Z\mid \theta) = \max_\theta \log p(X\mid \theta) \\ \theta^* = \max_\theta \sum_i \log \left( \sum_k \pi_k \mathcal{N}(x_i\mid \mu_k,\sigma_k) \right)\]

<p>Taking the derivative with respect to the \(\mu_k\), we find that</p>

\[\frac{\partial \cdots}{\partial \mu_k} = 0 = \sum_i \frac{\pi_k \mathcal{N}(x_i\mid \mu_k,\sigma_k)}{\sum_k \pi_k \mathcal{N}(x_i\mid \mu_k,\sigma_k) } \\  \sum_i q(z_i=k)   \Sigma_k^{-1}(x_i - \mu_k) =0 =  \sum_i q(z_i=k)   (x_i - \mu_k) \\ \boxed{\mu_k =\frac{\sum_i q(z_i=k) x_i}{\sum_i q(z_i=k)}}\]

<p>Similarly, we can find that</p>

\[\boxed{\Sigma_k =   \frac{ \sum_i q(z_i=k)(x_i-\mu)^\top(x_i-\mu)}{  \sum_i q(z_i=k)}  }\]

<h3 id="k-means-as-em">K-Means as EM</h3>

<p>Imagine the K-means model, where we randomly initialize \(\theta = \{\mu_1, \cdots, \mu_C\}\) points, and then we repeat the following steps until convergence</p>

<ul>
  <li>For each point we calculate the closest centroid</li>
</ul>

\[z_i = \text{argmin}_k \\mid x_i-\mu_k\\mid ^2\]

<ul>
  <li>Update centroid</li>
</ul>

\[\mu_k = \frac{\sum_{i:z_i=k} x_i}{\sum_{i:z_i=k} 1}\]

<p>The above K-means model can be think of as a GMM. Imagine we fix the covariance matrix to be identity, $\Sigma_k = I$, and also we fix the weights to be \(\pi_k = 1/\#\text{of Gaussians}\). We then will have</p>

\[p(x_i\mid z_i=k,\theta) = \frac{1}{Z} \exp\left(-\frac{1}{2} \\mid x_i-\mu_k\\mid ^2\right)\]

<p>Then, in the E-step, we pick \(q(z)\) such that they belong to the delta functions. Then are interested to find a function from the family of delta functions such that</p>

\[q(z_i) = \begin{cases} 1 &amp; \text{ if } z_i = \text{argmax}_k p(z_i=k\mid x_i,\theta)\\ 0 &amp; \text{otherwise}\end{cases}\]

<p>Note that</p>

\[p(z_i=k\mid x_i,\theta) = \frac{1}{Z} p(x_i\mid z_i,\theta) p(z_i\mid \theta) = \frac{1}{Z} \exp(-\frac{1}{2}\\mid  x_i-\mu_k\\mid ^2) \pi_k\]

<p>So the above maximization problem becomes</p>

\[q(z_i) = \begin{cases} 1 &amp; \text{ if } z_i = \text{argmin}_k  \\mid x_i-\mu_k\\mid ^2 \\ 0&amp; \text{otherwise}\end{cases}\]

<p>which is the same as the first step in the K-means. Now, lets look at the M-step, using the GMM derivation that we did above, we ha</p>

\[\mu_k =\frac{\sum_i q(z_i=k) x_i}{\sum_i q(z_i=k)} = \frac{\sum_{i:z_i=k} x_i}{\sum_{i:z_i=k} 1}\]

<h3 id="implementing-gmm-in-python">Implementing GMM in python</h3>

<p>So in GMM we are implementing the following formulae</p>

<p>E-step:</p>

\[q(z_i=k) = \frac{\pi_k \mathcal{N}(x_i\mid \mu_k,\Sigma_k)}{\sum_k\pi_k \mathcal{N}(x_i\mid \mu_k,\Sigma_k)}\]

<p>M-Step:</p>

\[\mu_k =\frac{\sum_i q(z_i=k) x_i}{\sum_i q(z_i=k)}\]

\[\Sigma_k =   \frac{ \sum_i q(z_i=k)(x_i-\mu)^\top(x_i-\mu)}{  \sum_i q(z_i=k)}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="c1"># here we denote q(z_i=k) with a NxC matrix called gamma
# where N is the number of poitns i=1,...,N
# and C is the number of clusters
</span><span class="k">def</span> <span class="nf">E_step</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">pi</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="s">"""
    Performs E-step on GMM model
    Each input is numpy array:
    X: (N x d), data points
    pi: (C), mixture component weights
    mu: (C x d), mixture component means
    sigma: (C x d x d), mixture component covariance matrices

    Returns:
    gamma: (N x C), probabilities of clusters for objects

    P(z\mid x) = (p(x\mid z) p(z))/(sum_z p(x\mid z) p(z) )
    gamma  =
    """</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># number of objects
</span>    <span class="n">C</span> <span class="o">=</span> <span class="n">pi</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># number of clusters
</span>    <span class="n">d</span> <span class="o">=</span> <span class="n">mu</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># dimension of each object
</span>    <span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span> <span class="c1"># distribution q(T)
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
      <span class="n">pi_i</span> <span class="o">=</span> <span class="n">pi</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
      <span class="n">gamma</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">pi_i</span><span class="o">*</span><span class="n">stats</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">,:],</span><span class="n">sigma</span><span class="p">[</span><span class="n">i</span><span class="p">,:,:]).</span><span class="n">pdf</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">gamma</span>

<span class="k">def</span> <span class="nf">M_step</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
    <span class="s">"""
    Performs M-step on GMM model
    Each input is numpy array:
    X: (N x d), data points
    gamma: (N x C), distribution q(T)

    Returns:
    pi: (C)
    mu: (C x d)
    sigma: (C x d x d)
    """</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># number of objects
</span>    <span class="n">C</span> <span class="o">=</span> <span class="n">gamma</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># number of clusters
</span>    <span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># dimension of each object
</span>
    <span class="n">resp_weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">pi</span> <span class="o">=</span> <span class="n">resp_weights</span><span class="o">/</span><span class="p">(</span><span class="mf">1.0</span><span class="o">*</span><span class="n">N</span><span class="p">)</span>

    <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">gamma</span><span class="p">.</span><span class="n">T</span><span class="p">,</span><span class="n">X</span><span class="p">)</span><span class="o">/</span><span class="n">resp_weights</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">C</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">d</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
      <span class="n">diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="o">-</span><span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
      <span class="n">weighted_sum</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">((</span><span class="n">gamma</span><span class="p">[:,</span><span class="n">i</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">diff</span><span class="p">).</span><span class="n">T</span><span class="p">,</span><span class="n">diff</span><span class="p">)</span>
      <span class="n">sigma</span><span class="p">[</span><span class="n">i</span><span class="p">,:,:]</span> <span class="o">=</span> <span class="n">weighted_sum</span><span class="o">/</span><span class="n">resp_weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">pi</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span>

<span class="k">def</span> <span class="nf">train_EM</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">restarts</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="s">'''
    Starts with random initialization *restarts* times
    Runs optimization until saturation with *rtol* reached
    or *max_iter* iterations were made.

    X: (N, d), data points
    C: int, number of clusters
    '''</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># number of objects
</span>    <span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># dimension of each object
</span>    <span class="n">best_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span>
    <span class="n">best_pi</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="n">best_mu</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="n">best_sigma</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">restarts</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">pi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">C</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
            <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">C</span><span class="p">,</span><span class="n">d</span><span class="p">)</span>
            <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">C</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">d</span><span class="p">))</span>
            <span class="n">sigma</span><span class="p">[...]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">identity</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
            <span class="n">prev_loss</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
              <span class="n">gamma</span> <span class="o">=</span> <span class="n">E_step</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">pi</span><span class="p">,</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">)</span>
              <span class="n">pi</span><span class="p">,</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">M_step</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">gamma</span><span class="p">)</span>
              <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_vlb</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">pi</span><span class="p">,</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">,</span><span class="n">gamma</span><span class="p">)</span>

              <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="p">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="ow">and</span> <span class="n">loss</span><span class="o">&gt;</span><span class="n">best_loss</span><span class="p">:</span>
                <span class="n">best_loss</span> <span class="o">=</span> <span class="n">loss</span>
                <span class="n">best_mu</span> <span class="o">=</span> <span class="n">mu</span>
                <span class="n">best_pi</span> <span class="o">=</span> <span class="n">pi</span>
                <span class="n">best_sigma</span> <span class="o">=</span> <span class="n">sigma</span>

              <span class="k">if</span> <span class="n">prev_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">prev_loss</span><span class="o">-</span><span class="n">loss</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">prev_loss</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">diff</span> <span class="o">&lt;</span> <span class="n">rtol</span><span class="p">:</span>
                  <span class="k">break</span>
              <span class="n">prev_loss</span> <span class="o">=</span> <span class="n">loss</span>

        <span class="k">except</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">LinAlgError</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Singular matrix: components collapsed"</span><span class="p">)</span>
            <span class="k">pass</span>

    <span class="k">return</span> <span class="n">best_loss</span><span class="p">,</span> <span class="n">best_pi</span><span class="p">,</span> <span class="n">best_mu</span><span class="p">,</span> <span class="n">best_sigma</span>
</code></pre></div></div>

<p>Benefits of GMM models: in unsupervised clustering for example, KNN methods accuracy will increase as we increase the number of groups. So you never know how many groups are better for KNN methods. In GNN on the other hand, the accuracy increases and then decreases. So increasing the number of clusters does not necessary increase the accuracy.</p>

<h3 id="dirichlet-distribution">Dirichlet Distribution</h3>

<p>A Dirichlet distribution is defined as</p>

\[f(\theta;\alpha) = \frac{1}{B(\alpha)} \Pi_{i=1}^K \theta_i^{\alpha_i-1}\]

<p>Note that \(\sum_{i=1}^K \theta_i = 1\) and \(\theta_i&gt;0\). The expected value and variance can be found as</p>

<p>Note that the Dirichlet distribution is conjugate to the multinomial distribution as</p>

\[p(\theta) = \frac{n!}{x_1! \cdots x_K!} \Pi_{i=1}^K \theta_i^{x_i}\]

<p>So if prior has a Dirichlet distribution, and likelihood is a multinomial, then the posetrior will also be a Dirichlet distribution.</p>

<h3 id="latent-dirichelet-model">Latent Dirichelet Model</h3>

<p>Imagine the following model for the distribution of words in a document</p>

<p><br /></p>
<div class="row mt-3" style="text-align:center;">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" width="300" src="/assets/research/bayes1.jpg" />
    </div>
</div>
<p><br /></p>

<p>In latent Dirichelet model, we have</p>

\[p(W,Z,\theta) = \Pi_{d=1}^D p(\theta_d) \Pi_{n=1}^{N_d} p(z_{dn}\mid \theta_d)~p(w_{dn}\mid z_{dn})\]

<p>where \(p(\theta_d)\sim \text{Dir}(\alpha)\), and \(p(z_{dn}\mid \theta_d) = \theta_{dz_{dn}}\), and \(p(w_{dn}\mid z_{dn}) = \Phi_{z_{dn},w_{dn}}\), where \(\sum \Phi_{z_{dn},w_{dn}} =1\). In order to calculate the E-step, we first need to find the log-likelihood, where we have</p>

\[\log p(W,Z,\theta) = \sum_{d=1}^D \left[ \sum_{t=1}^T (\alpha_t-1) \log \theta_{dt} +  \sum_{n=1}^{N_d} \sum_{t=1}^T \mathbf{1}(z_{dn}=t) \left(\log \theta_{dt} + \log \phi_{tw_{dn}} \right) \right] + \text{C.}\]

<p>In the E-step, we want to</p>

\[\min D_{KL}\left( q(\theta) q(Z) \\mid  p(\theta,Z\mid W) \right)\]

\[\log q(\theta) = \mathbf{E}_{q(z)} \log p(\theta,Z,W)  =  \mathbf{E}_{q(z)} \log p(\theta,Z\mid  W) + C.\]

\[\log q(\theta) = \mathbf{E}_{q(z)} \sum_{d=1}^D \left[ \sum_{t=1}^T (\alpha_t-1) \log \theta_{dt} +  \sum_{n=1}^{N_d} \sum_{t=1}^T\mathbf{1}(z_{dn}=t) \left(\log \theta_{dt} + \log \phi_{tw_{dn}} \right) \right]\\ = \mathbf{E}_{q(z)} \sum_{d=1}^D \left[ \sum_{t=1}^T (\alpha_t-1) \log \theta_{dt} +  \sum_{n=1}^{N_d}\sum_{t=1}^T \mathbf{1}(z_{dn}=t) \left(\log \theta_{dt} \right) \right] \\ =  \sum_{d=1}^D \left[ \sum_{t=1}^T (\alpha_t-1) \log \theta_{dt} +  \sum_{n=1}^{N_d} \sum_{t=1}^T \mathbf{E}_{q(z)}\left[\mathbf{1}(z_{dn}=t)\right] \left(\log \theta_{dt}  \right) \right] \\  =  \sum_{d=1}^D \sum_{t=1}^T \log \theta_{dt} \left[ (\alpha_t-1)  +  \sum_{n=1}^{N_d} \mathbf{E}_{q(z)}\left[\mathbf{1}(z_{dn}=t)\right] \right]\]

<p>So in summary, we have</p>

\[\boxed{\log q(\theta) =  \sum_{d=1}^D \sum_{t=1}^T \log \theta_{dt} \left[ (\alpha_t-1)  +  \sum_{n=1}^{N_d} \gamma_{dn}^t \right]} \\ \boxed{\gamma_{dn}^t = \mathbf{E}_{q(z)}\left[\mathbf{1}(z_{dn}=t)\right]}\]

\[\to q(\theta) \propto \Pi_d \Pi_t \theta_{dt}^{\left[\alpha_t + \sum_n \gamma^t_{dn} - 1\right]}\]

<p>Now, let’s take the E-step for \(q(Z)\), we have</p>

\[\log q(Z) = \\  \mathbf{E}_{q(\theta)} \sum_{d=1}^D \left[ \sum_{t=1}^T (\alpha_t-1) \log \theta_{dt} +  \sum_{n=1}^{N_d} \sum_{t=1}^T\mathbf{1}(z_{dn}=t) \left(\log \theta_{dt} + \log \phi_{tw_{dn}} \right) \right] \\ = \mathbf{E}_{q(\theta)} \sum_{d=1}^D  \sum_{n=1}^{N_d} \mathbf{1}(z_{dn}=t) \left(\log \theta_{dt} + \log \phi_{tw_{dn}} \right)  \\ =  \sum_{d=1}^D  \sum_{n=1}^{N_d} \mathbf{1}(z_{dn}=t) \left(\mathbf{E}_{q(\theta)} \log \theta_{dt} + \log \phi_{tw_{dn}} \right)\]

<p>so in summary</p>

\[\boxed{\log q(Z) = \sum_{d=1}^D  \sum_{n=1}^{N_d} \mathbf{1}(z_{dn}=t) \left(\mathbf{E}_{q(\theta)} \log \theta_{dt} + \log \phi_{tw_{dn}} \right) } \\ \to q(Z) = \Pi_d \Pi_t q(z_{dn})\\ q\left( z_{dn}=t\right) =   \frac{\phi_{t w_{dn}} \exp \left( \mathbf{E}_{q(\theta)} \log \theta_{dt} \right)}{\sum_{t'} {\phi_{t' w_{dn}} \exp \left( \mathbf{E}_{q(\theta)} \log \theta_{dt'} \right)}}\]

<p>and in the M-step we would like to maximize the following</p>

\[\mathbf{E}_{q(\theta)q(Z)} \log p(\theta,Z,W)  = \mathbf{E}_{q(\theta)q(Z)}  \left[ \sum_{d=1}^D \left[ \sum_{t=1}^T (\alpha_t-1) \log \theta_{dt} +  \sum_{n=1}^{N_d} \sum_{t=1}^T \mathbf{1}(z_{dn}=t) \left(\log \theta_{dt} + \log \phi_{tw_{dn}} \right) \right] \right] \\ = \mathbf{E}_{q(\theta)q(Z)}  \left[ \sum_{d=1}^D \sum_{t=1}^T \sum_{n=1}^{N_d} \mathbf{1}(z_{dn}=t)  \log \phi_{tw_{dn}} \right] \\  =   \sum_{d=1}^D \sum_{t=1}^T \sum_{n=1}^{N_d} \mathbf{E}_{q(\theta)q(Z)} \left[\mathbf{1}(z_{dn}=t)\right]  \log \phi_{tw_{dn}} \\ =   \sum_{d=1}^D \sum_{t=1}^T \sum_{n=1}^{N_d} \gamma_{dn}^t  \log \phi_{tw_{dn}}\]

<p>given that \(\sum_w \phi_{tw} = 1\) and also \(\phi_{tw}\geq 0\). We use lagrangian to maximize the above equation, we have</p>

\[L =  \sum_{d=1}^D \sum_{t=1}^T \sum_{n=1}^{N_d} \gamma_{dn}^t \log \phi_{tw_{dn}} - \sum_{t=1}^T \lambda_t \left(\sum_w \phi_{tw}-1\right)\]

<p>Now we take the derivative to maximize the above equation, we find</p>

\[\frac{\partial L}{\partial \phi_{tw}} = \sum_{d=1}^D \sum_{n=1}^{N_d} \gamma_{dn}^t \frac{1}{\phi_{tw}}\mathbf{1}\left[w_{dn} = w\right] -  \lambda_t  = 0 \\ \phi_{tw} = \frac{\sum_{d=1}^D \sum_{n=1}^{N_d} \gamma_{dn}^t \mathbf{1}\left[w_{dn}= w\right]}{ \lambda_t}\]

<p>Knowing that \(\sum_w \phi_{tw} = 1\), we can find that</p>

\[\phi_{tw} = \frac{\sum_{d=1}^D \sum_{n=1}^{N_d} \gamma_{dn}^t \mathbf{1}\left[w_{dn}= w\right]}{ \sum_w \sum_{d=1}^D \sum_{n=1}^{N_d} \gamma_{dn}^t \mathbf{1}\left[w_{dn}= w\right]} \\ \to \boxed{\phi_{tw} = \frac{\sum_{d,n} \gamma_{dn}^t \mathbf{1}\left[w_{dn}= w\right]}{ \sum_{d,n,w'} \gamma_{dn}^t \mathbf{1}\left[w_{dn}= w'\right]}}\]

<h2 id="monte-carlo-method">Monte-Carlo Method</h2>

<p>Estimating expected values using sampling</p>

\[\mathbf{E}_{p(x)} f(x) = \frac{1}{M} \sum f(x_s), \quad \text{where } x_s\sim p(x)\]

<p>Now the question is how to sample from a probability distribution \(p(x)\)? In the following we will discuss this. Note that we assume that generating a random number with uniform distribution from in \([0,1]\) is given, i.e., we can easily sample form \(\mathcal{U}(0,1)\).</p>

<h3 id="sampling-from-1-d-distribution">Sampling from 1-D distribution</h3>

<p>Consider a distribution over discrete set such as \(p(a_i) = p_i\) for \(i=1,\cdots,n\). Note that \(\sum_i p_i = 1\). We can separate the \([0,1]\) distance proportional to the \(p_i\). Next, we sample a point from $[0,1]$ using the uniform distribution, we can assign it to the discrete values of \(a_i\) based on the interval that it lands into. Here is an example:</p>

<p><br /></p>
<div class="row mt-3" style="text-align:center;">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" width="300" src="/assets/research/bayes2.png" />
    </div>
</div>
<p><br /></p>

<p>Sampling Normal distribution:</p>

<p>We can generate normal distribution using central limit theorem, i.e.</p>

\[x = \frac{1}{\sqrt{N}} \left[ \sum_{i=1}^N \left( x_i - \frac{1}{2}\right) \right]\]

<p>As the \(N\to \infty\), the \(p(x) \to \mathcal{N}(0,1)\). This has been done very efficiently, and we can use packages such as <code class="language-plaintext highlighter-rouge">np.random.randn()</code> to generate these numbers. Now imagine that we are interested in 1d sampling from a continuous distribution such as \(p(x)\) shown below</p>

<p><br /></p>
<div class="row mt-3" style="text-align:center;">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" width="300" src="/assets/research/bayes3.png" />
    </div>
</div>
<p><br /></p>

<p>One way to sample from \(p(x)\) in the above is to first bound the pdf by some normal distribution $q(x)$. Next, we generate a random point, say \(x_0\). We then accept this point with probability \(\alpha= p(x)/q(x)\) and reject it with \(1-\alpha\). This way, we create samples from the \(p(x)\).</p>

<h3 id="markov-chains">Markov-Chains</h3>

<p>There are two methods that we will introduce here (Metropolis-Hasting and Gibbs sampling) that depend on Markov-Chain. A few introductory remarks on Markov-process is helpful before we talk about them.</p>

<p>Let \(X_t\) denote the value of a random variable at time $t$. The <em>state space</em> is the space of all possible values for $X$ values. The random variable is called a <strong>Markov process</strong> if the transition probabilities betwen different values in the state space only depend on the current’s value of the random variable, i.e.,</p>

\[p(X_{t+1}=s_{j}\mid X_0=s_k, \cdots, X_t=s_i) = p(X_{t+1}=s_{j}\mid  X_t=s_i)\]

<p>A Markov-Chain referes to a sequence of random variables \((X_0, \cdots,X_n)\) generated by a markov process. Transition probability (or the transition kernel) gives us the probability of transitioning between the states ina single step, i.e.,</p>

\[p(i\to j) = p(X_{t+1}=s_j\mid  X_{t}=s_i)\]

<p>Using the transition kernel, if we are at state \(s_i\) we can define a row vector for the state space probabilities, i.e., \(\pi_i(t) = p(X_t = s_i)\). The evolution of this state space probabilities can be obtained using the kernel as</p>

\[\pi_j(t+1) = p(X_{t+1}=s_j) \\= \sum_k p(X_{t+1}=s_j\mid   X_t = s_k) p(X_t = s_k) = \sum_k p(k,j) \pi_k(t) \\ \mathbf{\pi}(t+1) = \mathbf{\pi}(t) \mathbf{P}\]

<p>As a result, we can find that \(\pi(t) = \pi(0) \mathbf{P}^k\). A distribution of states $\mathbf{\pi}^*$ is called stationary if \(\mathbf{\pi}^* = \mathbf{\pi}^*\mathbf{P}\). A sufficient condition for a unique stationary distribution is that the <strong>detailed balance</strong> condition holds</p>

\[p(j\to k) \pi^*_j = p(k\to j) \pi^*_k\]

<p>If the above condition holds then we have</p>

\[(\mathbf{\pi}\mathbf{P})_j = \sum_i \pi_i P(i,j) = \sum_i \pi_i P(i\to j) \\= \sum_i \pi_j P(j\to i)  = \pi_j \sum_i P(j\to i) = \pi_j\]

<h3 id="metropolis-hasting-algorithm">Metropolis-Hasting Algorithm</h3>

<p>So our goal is to generate a sample with PDF \(p(x)\). In Metropolis-Hasting the basic idea is to create a Markov-Process to generate new data points where \(p(x)\) is its stationary distribution. If \(p(x)\) is stationary distribution, then using following the Markov process for a long time we will generate data-points that have the distribution of \(p(x)\). But how can we create a Markov-Process where \(p(x)\) is its stationary distribution? We can use the idea of rejecting points. We start with any Markov-process \(Q\) (as long as we have a non-zero probability of going over all the data points), we then start from a data-point \(x_0\) (or state) and find our new data-point \(x_1\) (the new state). Now we have an option of accepting this new state or rejecting it. We select this acceptance/rejection probability such that our \(p(x)\) becomes the stationary distribution of our Markov process.</p>

\[p(x_0) Q(x_0\to x_1) A(x_0 \to x_1) = p(x_1) Q(x_1\to x_0) A(x_1 \to x_0) \\ \frac{A(x_0 \to x_1)}{A(x_1 \to x_0)} = \frac{p(x_1)}{p(x_0)} \frac{Q(x_1\to x_0)}{Q(x_0\to x_1)} = \rho\]

<p>So as long as our acceptance probability follows the above equality, we are doing well. Assume that the above proportionality is \(\rho\). If \(\rho\leq 1\), we can have \(A(x_0\to x_1) = \rho, A(x_1\to x_0) = 1\). On the other hand, if \(\rho&gt;1\), we can assign \(A(x_0\to x_1) = 1, A(x_1\to x_0) = 1/\rho\). So basically, we can assign the following acceptance probability</p>

\[A(x_0\to x_1) = \begin{cases} \rho &amp; \rho \leq 1 \\ 1 &amp; \rho &gt;1 \end{cases}\]

<p>Or we can summarize it as</p>

\[A(x_0\to x_1) = \min\left(1, \frac{p(x_1)}{p(x_0)} \frac{Q(x_1\to x_0)}{Q(x_0\to x_1)} \right)\]]]></content><author><name></name></author><summary type="html"><![CDATA[Machine learning]]></summary></entry><entry><title type="html">Optimum first guess for Persian Wordle</title><link href="https://azareei.github.io/blog/2022/wordle/" rel="alternate" type="text/html" title="Optimum first guess for Persian Wordle" /><published>2022-01-26T08:30:00+10:00</published><updated>2022-01-26T08:30:00+10:00</updated><id>https://azareei.github.io/blog/2022/wordle</id><content type="html" xml:base="https://azareei.github.io/blog/2022/wordle/"><![CDATA[<p><a href="vajoor.ir">Vaajoor</a> is the persian version of <a href="https://www.powerlanguage.co.uk/wordle/">wordle</a>. In this post I explore the optimum first guess for <a href="vajoor.ir">Vaajoor</a> in persian.</p>

<p><br /></p>

<p style="text-align:right;dir:rtl;">
در بازی واجور هدف حدس زدن یک کلمه پنج حرفیه. در این بازی امکان پنج‌بار حدس زدن وجود داره. برای هر کلمه‌ای که حدس میزنیم، برای هر حرف سه حالت وجود داره: اگر حرف در کلمه اصلی وجود نداشته باشه، حرف قرمز میشه. اگر حرف در کلمه اصلی وجود داشته باشد، اما مکانش درست نباشه، زرد میشه. و اگر حرف هم در کلمه اصلی باشه و هم مکانش درست باشه، سبز میشه. خیلی شبیه به بازی فکر‌بکر هست اگه توی بچگی بازی کرده باشین. حال با حدس کلمه و اینکه هر حرف‌ آن در چه حالتی است، کلمه اصلی حدس زده میشه 
</p>

<p><br />
<br /></p>

<p style="text-align:right;dir:rtl;">
نمونه‌ای از بازی
</p>

<div class="row mt-3" style="text-align:center;">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" width="300" src="/assets/posts/vajoor_sample.png" />
    </div>
</div>

<p><br />
<br /></p>

<p style="text-align:right;dir:rtl;">

سوالی که پیش میآید این هست که آیا حدس اول بهینه‌ای وجود داره؟

<br />

با نگاهی به لغات ۵ حرفی فرهنگ معین میبینیم که تعداد لغات پنج‌حرفی ۸۲۱۶ کلمه هست. حدس اول میتونه از کل این لیست باشه. با توجه به حدس اول، و اینکه چه حرف‌هایی در آن هست و نیست لیست کلمات قابل قبول برای حدس دوم کمتر میشه. سوال این هست که آیا حدس اولیه بهینه‌ای وجود داره؟
<br />

هر حدسی که ما میزنیم، بقیه کلمات رو به ۳*۳*۳*۳*۳ دسته تقسیم میکنه، با توجه به اینکه هر حرف این حدس ما در کدوم گروه سبز یا زرد یا قرمز میافته. پس هر کلمه‌ای، بقیه کلمات رو بین این ۳*۳*۳*۳*۳ = ۲۴۳ گروه تقسیم میکنه. حالا با توجه به اینکه کدوم گروه انتخاب شده، ادامه مسیر از بین کلمات اون گروه خواهد بود. حالا اگه حدس اولیه‌ای بهینه باشه، باید کلمات رو جوری بین این گروه‌ها تقسیم کنه، که لیست تعداد کلمات بعدی کمترین باشه! اگه اینجوری باشه، این کلمه انتخاب خوبی بوده
<br />

برای اینکه کلمه بهینه برای انتخاب اول رو پیدا کنیم، روی لیست تمام کلمات میگردیم، برای هر کلمه نگاه میکنیم که بقیه کلمات چه‌طور بین این ۲۴۳ گروه تقسیم میشن. برای هر گروه تعدادی ثبت میشه. ماکزیمم تعداد کلمات در یه گروه از این ۲۴۳ گروه رو برای اون کلمه ثبت میکنیم. در نهایت کلمه‌ای رو انتخاب میکنیم که این عدد ماکزیممش کمترین باشه. یعنی مینیمم این ماکزیمم رو پیدا میکنیم. در این صورت، در بدترین حالت حدسمون، تعداد کلمات برای انتخاب دوم کمترین شده
<br />

حالا همین کار رو من برای کلمات فارسی فرهنگ معین انجام دادم (کد شخمی نوشته شده پایین همین صفحه هست). به نظر میاد که حدس بهینه اول کلمه تریان هست. منم نمیدونستم تریان یعنی چه ! انگار به معنی چیزی هست که طبقی شکل هست و بافته شده از شاخ بید! والا منم نفهمیدم یعنی چی. حالا اگه کلمه تریان رو به عنوان انتخاب اول بزاریم، ماکزیمم تعداد کلمات در اون ۲۴۳ گروه فقط ۵۴۲ کلمه هست. یعنی در بد‌ترین حالت برای حدس دوم باید فقط از بین این ۵۴۲ کلمه انتخاب کنیم. حالا همین داستان رو برای حدس دوم هم میشه انجام داد، البته وقتی حدس اول در اومده باشه

<br />

حالا چون تریان خیلی واژه سختی شد، کلمه دوم هم نگاه کردم دیدم خیلی ساده‌تره: تیمار. هم کلمه راحت‌تری هست و هم آدم یادش بمونه. اگه کلمه تیمار رو برای حدس اول انتخاب کنین، برای حدس دوم در بدترین حالت فقط باید بین ۵۶۰ کلمه، اون کلمه مورد نظر رو پیدا کنین. در ادامه هم لینک گیت‌هاب این کد سر‌سری پایتون و لیست کلمات فرهنگ معین رو میزارم خواستین خودتون باش ور برین

</p>
<p><br />
<br /></p>

<p><a href="https://github.com/ahmadzareei/vajoor">Github Link</a></p>

<p><br />
<br /></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'Moin_dictionary_words.txt'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
<span class="n">lines</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">readlines</span><span class="p">()</span>

<span class="n">lines</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>

<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">lines</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">==</span><span class="mi">5</span><span class="p">]</span>

<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="n">score</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>\<span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
<span class="k">for</span> <span class="n">j</span><span class="p">,</span><span class="n">w</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">):</span>
<span class="n">category</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="k">for</span> <span class="n">w_inquiry</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
<span class="n">key</span> <span class="o">=</span> <span class="s">''</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">ch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
<span class="k">if</span> <span class="n">ch</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">w_inquiry</span><span class="p">:</span>
<span class="n">key</span> <span class="o">+=</span> <span class="s">'K'</span> <span class="c1"># it's black
</span><span class="k">else</span><span class="p">:</span>
<span class="k">if</span> <span class="n">ch</span> <span class="o">==</span> <span class="n">w_inquiry</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
<span class="n">key</span> <span class="o">+=</span> <span class="s">'G'</span> <span class="c1"># it should be green
</span><span class="k">else</span><span class="p">:</span>
<span class="n">key</span> <span class="o">+=</span> <span class="s">'Y'</span> <span class="c1"># then it's yellow
</span><span class="n">category</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="n">score</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">category</span><span class="p">.</span><span class="n">values</span><span class="p">())</span>

<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">score</span><span class="p">).</span><span class="n">argsort</span><span class="p">()[:</span><span class="mi">20</span><span class="p">]:</span>
<span class="k">print</span><span class="p">(</span><span class="n">score</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span><span class="n">words</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p style="text-align:right;dir:rtl;">
542 تریان
<br />
560 تیمار
<br />
611 رمانی
<br />
631 نوایر
<br />
631 روانی
<br />
643 ربانی
<br />
643 برانی
<br />
644 منابر
<br />
656 نرمال
<br />
661 بهیار
<br />
661 بیراه
<br />
663 ویران
<br />
663 رویان
<br />
680 تریاک
<br />
681 مهوار
<br />
681 میراب
<br />
681 هموار
<br />
681 بیمار
<br />
686 نهاری
<br />
686 مناره
</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Molavi]]></summary></entry><entry><title type="html">KL Divergence between two normal distributions</title><link href="https://azareei.github.io/blog/2021/KL-Divergence/" rel="alternate" type="text/html" title="KL Divergence between two normal distributions" /><published>2021-03-11T08:10:00+10:00</published><updated>2021-03-11T08:10:00+10:00</updated><id>https://azareei.github.io/blog/2021/KL-Divergence</id><content type="html" xml:base="https://azareei.github.io/blog/2021/KL-Divergence/"><![CDATA[<p>These is a review of Bayesian Machine Learning. It’s based on HSE’s course on this topic.</p>

<h3 id="maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</h3>

<p>The ML estimate for \(\theta\) is the value under which the data are most likely, i.e.</p>

\[\boxed{\hat{\theta}_{ML} \in \text{argmax}_\theta p(y|\theta)}\]

<p>Example: We know that a fair coin has \(p(\text{head}) = p(\text{tail}) = 1/2\). Now imagine you have a coin and you don’t know if it is fair or not. You decide to throw the coin \(m\) times, where you observe \(m_H\) times head, and \(m_T\) times tail, where \(m_H + m_T = m\). Now you ask given this observation, what is the probability of \(p(\text{head}), p(\text{tail})\) for this coin? We know that if \(p(\text{head}) =\theta\), then the likelihood of such observation is</p>

\[p(y|\theta) = \theta^{m_H} (1-\theta)^{m-m_H}\]

<p>Now, we are interested to find a \(\hat \theta_{ML}\) such that the above probability is maximized. Instead of the probability, we maximize its logarithm which will be easier. We have</p>

\[l(\theta) = m_H \log(\theta) + (m-m_H) \log(1-\theta)\]

<p>finding the \(\text{argmax}_\theta\) for the above equation we have</p>

\[\frac{d}{d\theta}d(\theta)|_{\hat\theta} = 0 = \frac{m_H}{\theta}|_{\hat\theta} - \frac{m-m_H}{1-\theta}|_{\hat\theta}\]

\[\longrightarrow \hat{\theta} = \frac{m_H}{m}\]

<h3 id="maximum-a-posteriori-map-estimation">Maximum a posteriori (MAP) estimation</h3>

<p>We learned that in MLE estimation we find \(\theta\) to maximize the likelihood function \(p(y\mid \theta)\). Now, in MAP, we find \(\theta\) such that the posterior \(p(\theta\mid y)\) is maximized, i.e.,</p>

\[\boxed{\hat{\theta}_{MAP} \in \text{argmax}_\theta p(\theta\mid y) = \text{argmax}_\theta  \frac{p(y\mid \theta) p(\theta)}{p(y)}}\]

<p>Since the denominator does not depend on \(\theta\), we have</p>

\[\hat{\theta}_{MAP} = \text{argmax}_\theta p(y\mid \theta)p(\theta) = \text{argmax}_\theta \log p(y\mid \theta) + \log p(\theta)\]

<p>The last term, \(\log p(\theta)\) is known as our prior belief.</p>

<p>Example: Again, imagine the same coin flip problem. Imagine we take the prior to be a beta distribution as</p>

\[p(\theta) = \beta(\theta;\alpha,\beta) = A~ \theta^\alpha (1-\theta)^{\beta}\]

<p>where \(A\) is a constant to make \(\int p(\theta)\text{d}\theta=1\). Now finding the solution to the MAP estimator, we find that</p>

\[\hat{\theta}_{MAP} = \text{argmax}_\theta \log p(y\mid \theta) + \log p(\theta)\]

\[\frac{m_H}{\theta}\mid _{\hat\theta} - \frac{m-m_H}{1-\theta}\mid _{\hat\theta}  + \frac{\alpha}{\theta}\mid _{\hat\theta} - \frac{\beta}{1-\theta}\mid _{\hat\theta} = 0\]

\[\hat{\theta}_{MAP} = \frac{m_H + \alpha}{m+\alpha+\beta }\]

<p>The point of \(\alpha, \beta\) coming from the prior in the above equation will be a regularizer. Imagine, we only throw the coin only once, then the ML gives \(\hat{\theta}_{ML}=1\), however, \(\hat{\theta}_{MAP} = \alpha/(\alpha+\beta)\).</p>

<h3 id="point-estimation-and-probabilistic-linear-regression">Point estimation and probabilistic linear regression</h3>

<p>Imagine we are given $m$ datapoints, \((x_1,y_1), \cdots, (x_m,y_m)\), where \(x_i\)’s are independent variables and \(y_i\)’s are dependent. Assuming a linear relationship as</p>

\[y_i \sim \theta^\top x_i + \epsilon\]

<p>where \(\epsilon \sim \mathcal{N}(0,\sigma^2)\). Assuming the independence between the \(y_i\)’s, we can write</p>

\[p(y\mid x,\theta)= \Pi_{i=1}^m p(y_i\mid x_i,\theta) = \Pi_{i=1}^m \mathcal{N}(y_i;\theta^\top x_i,\sigma^2)\]

<p>Finding the maximum of log-likelihood, we obtain</p>

\[\hat{\theta}_{ML} = \text{argmax}_\theta p(y\mid x,\theta) = \text{argmin}_\theta \sum_{i=1}^m (y_i - \theta^\top x_i)^2\]

<p>which is the equivalent to ordinary least squares.</p>

<p>Now, trying the MAP estimate, we first need to assume a prior for the \(\theta\) variables. As an initial estimate we assume \(\theta \sim \mathcal{N}(0, \nu^2\mathbf{I})\). Finding the MAP solution we find that</p>

\[\hat{\theta}_{MAP} = \text{argmax}_\theta \log p(y\mid \theta) + \log p(\theta)\]

\[\hat{\theta}_{MAP} = \text{argmin}_\theta \sum_{i=1}^m (y_i-\theta^\top x_i)^2 + \frac{\sigma^2}{\nu^2} \mid \mid \theta\mid \mid ^2\]

<p>which is the same as ML solution with a regularization term.</p>

<h3 id="conjugate-priors">Conjugate priors</h3>

<p>We often would like to find the full posterior, \(p(\theta\mid y)\). If we know the full posterior distribution, then we can do posterior predictive distribution as</p>

\[p(y_{m+1}\mid y) = \int p(y_{m+1}\mid y,\theta)~p(\theta\mid y) d\theta = \int p(y_{m+1}\mid \theta)~p(\theta\mid y) d\theta\]

<p>The second part is because \(y_{m+1}\) and \(y\) are conditionally independent. If we would like to find the full posterior, then</p>

\[p(\theta\mid y) = \frac{p(y\mid \theta)p(\theta)}{p(y)}\]

<p>The bottom part is just a normalizer. So we need to choose the prior, \(p(\theta)\), such that with the likelihood distribution, \(p(y\mid \theta)\), we are able to track the distribution. Selecting prior in such a way that the \(p(y\mid \theta)p(\theta)\) becomes tractable is known as a selection of conjugate priors for \(p(y\mid \theta)\).</p>

<h3 id="exponential-families-and-conjugate-priors">Exponential families and conjugate priors</h3>

<p>The family of distribution \(\mathcal{F}\) is called exponential family if every member of \(\mathcal{F}\) has the form</p>

\[\boxed{p(y_i\mid \theta) = f(y) g(\theta)\exp(\phi(\theta)^\top u(y_i))}\]

<p>where $f(\cdot), g(\cdot), \phi(\cdot),u(\cdot)$ are some functions. For example, an exponential distribution is an exponential family distribution:</p>

\[p(y\mid \theta) = \theta e^{-\theta y}\]

<p>or a beta distribution:</p>

\[p(y\mid \theta) = \theta^{y} (1-\theta)^{1-y} = \exp(y\log\theta + (1-y)\log(1-\theta)) = (1-\theta)\log(y \log\frac{\theta}{1-\theta})\]

<p>Normal distribution is also an exponential family.</p>

<p>If the \(p(y_i\mid \theta)\) is coming from an exponential family distribution, then if \(y_i\)’s are independent, the likelihood becomes</p>

\[p(y\mid \theta) = \Pi_{i=1}^m p(y_i\mid \theta) = \left[\Pi_{i=1}^m f(y_i)\right] g(\theta)^m \exp\left( \phi(\theta)^\top \sum_{i=1}^m u(y_i)\right)\]

<p>Now, we can select the prior as</p>

\[p(\theta) \sim g(\theta)^\eta \exp(\phi(\theta)^\top \nu)\]

<p>where \(\eta,\nu\) are hyper-parameters.</p>

<p>Now you might ask, doesn’t the choice of prior doesn’t matter? Can we really pick any prior distribution? <strong>In the next part, we show that as long as you pick a prior that assigns non-zero probabilities to every possible value of $\theta$, the solution of MAP converges to the true solution $\theta^<em>$, or more precisely, $\theta^</em>$ corresponds to the likelihood model which is closest to the true generating distribution. The closeness of the distributions is defined using KL divergence.</strong></p>

<h3 id="kl-divergence">KL divergence</h3>

<p>The KL divergence between two distributions \(p\) and \(q\) is defined as</p>

\[\boxed{D_{KL}(p\mid \mid q) := \int_x p(x) \log \frac{p(x)}{q(x)}dx}\\ \to D_{KL}(p\mid \mid q)= \mathbf{E}_p[\log p(x) - \log q(x)]\]

<p>Note that \(D_{KL}(p\mid \mid q)\) is always positive since</p>

\[-D_{KL}(p\mid \mid q) = \int p(x)\left(-\log \frac{p(x)}{q(x)}\right) dx \leq  -\log \int p(x) \frac{q(x)}{p(x)} dx =0\]

\[\rightarrow D_{KL}(p\mid \mid q)\geq 0\]

<p>and the equality with zero only happens when \(p=q\). Now, we want to use the KL divergence to find the distribution from the likelihood family that is closest to the true generating function</p>

\[\theta^* = \text{argmin}_\theta D(q(\cdot) \mid \mid p(\cdot\mid \theta))\]

<p>Imagine we have \(y=(y_1,\cdots, y_m)\) as a set of independent samples from an arbitrary distribution \(q(y)\). We assume a family \(\mathcal{F}\) of likelihood distributions with \(\Theta = \{ \theta: p(\cdot \mid  \theta) \in \mathcal{F}\}\) the finite parameter space. Our goal is to show that if \(p(\theta=\theta^*)&gt;0\), then \(p(\theta=\theta^*\mid y)\to1\) as the number of observation increases or \(m\to\infty\). Consider \(\theta\neq \theta^*\), then</p>

\[\log \frac{p(\theta\mid y)}{p(\theta^*\mid y)} = \log \frac{p(y\mid \theta) p (\theta)}{p(y\mid \theta^*) p (\theta^*)} = \log \frac{p (\theta)}{p (\theta^*)} + \sum_{i=1}^m \log \frac{p(y_i\mid \theta) }{p(y_i\mid \theta^*)}\]

<p>where we used the fact that \(p(\theta^*) \neq 0\). We have</p>

\[\frac{1}{m} \sum_{i=1}^m \log \frac{p(y_i\mid \theta) }{p(y_i\mid \theta^*)} \to \mathbf{E}_q\left[ \log \frac{p(y_i\mid \theta) }{p(y_i\mid \theta^*)}\right]\]

<p>Expanding the Expected value we have</p>

\[\mathbf{E}_q\left[ \log \frac{p(y_i\mid \theta) }{p(y_i\mid \theta^*)}\right] = \mathbf{E}_q\left[ \log \frac{p(y_i\mid \theta) }{p(y_i\mid \theta^*)}\frac{q(y)}{q(y)}\right] = \mathbf{E}_q\left[ \log \frac{q(y) }{p(y_i\mid \theta^*)}- \log \frac{q(y)}{p(y_i\mid \theta)} \right]\]

\[\mathbf{E}_q\left[ \log \frac{p(y_i\mid \theta) }{p(y_i\mid \theta^*)}\right] = D_{KL}\left(q(\cdot) \mid \mid  p(\cdot\mid \theta^*)\right) - D_{KL}\left(q(\cdot) \mid \mid  p(\cdot\mid \theta)\right) &lt; 0\]

<p>The above result is negative since we assume that \(\theta^*\) minimizes the KL divergence between \(q(\cdot)\) and \(p(\cdot\mid \theta)\). So far we have found that</p>

\[\sum_{i=1}^m \log \frac{p(y_i\mid \theta) }{p(y_i\mid \theta^)} \to m\mathbf{E}_q\left[ \log \frac{p(y_i\mid \theta) }{p(y_i\mid \theta^)}\right] \to -\infty\]

<p>since the expected value we found to be negative, and as \(m\to\infty\), the value goes to negative infinity. Plugging back into the initial equation we have</p>

\[\log \frac{p(\theta\mid y)}{p(\theta^*\mid y)} = \log \frac{p (\theta)}{p (\theta^*)} + \sum_{i=1}^m \log \frac{p(y_i\mid \theta) }{p(y_i\mid \theta^*)} = \log \frac{p (\theta)}{p (\theta^*)} -\infty \to -\infty\]

\[\log \frac{p(\theta\mid y)}{p(\theta^*\mid y)}  \to -\infty\]

<p>This implies that \(p(\theta\mid y)/p(\theta^*\mid y) \to 0\), which means that \(p(\theta\mid y) \to 0\). We started with the fact that \(\theta\neq\theta^*\). So if \(p(\theta\mid y)\to 0\) for \(\theta\neq\theta^*\), then \(p(\theta^*\mid y)\to 1\).</p>

<h1 id="expectation-maximization">Expectation Maximization</h1>

<h3 id="guassian-mixture-models">Guassian Mixture Models:</h3>

<p>In Gaussian Mixture model, you assume that your data is coming from a combination of Gaussian (Gaussian Mixture). There is a latent variable, \(z\), that determines which Gaussian to pick or how to combine the Gaussian models. In this latent model, we are interested to find \(\theta\) parameters of the Gaussian, such that \(p(x\mid \theta)\) is maximized. Let’s assume that the latent variable is discrete and \(z=1\) or \(2\). The probability of observing a datapoint is</p>

\[p(x\mid \theta) = \sum_{c=1}^2 p(x,z=c\mid \theta) = \sum_{c=1}^2 p(z=c)~p(x\mid \theta,z)\]

<p>Our goal as usual is to find \(\max_\theta p(X\mid \theta)\). We have</p>

\[\max_\theta p(X\mid \theta) = \max_\theta \log p(X\mid \theta)= \max_\theta \log \Pi_{i=1}^N p(x_i\mid \theta)=  \max_\theta \sum_{i=1}^N \log p(x_i\mid \theta)\]

\[\max_\theta p(X\mid \theta) = \max_\theta \sum_{i=1}^N \log \sum_{c=1}^2 p(x_i,z_i=c\mid \theta) = \max_\theta \sum_{i=1}^N \log \sum_{c=1}^2 \frac{q(z_i=c)}{q(z_i=c)}p(x_i,z_i=c\mid \theta) = \max_\theta \sum_{i=1}^N \log \sum_{c=1}^2q(z_i=c) \frac{p(x_i,z_i=c\mid \theta)}{q(z_i=c)}  \geq \max_\theta \sum_{i=1}^N \sum_{c=1}^2 q(z_i=c) \log \frac{p(x_i,z_i=c\mid \theta)}{q(z_i=c)}\]

<p>So, in summary we have found the following</p>

\[\boxed{\log p(X\mid \theta) \geq \mathcal{L}(\theta,q) \text{ for any } q }\\ \boxed{\mathcal{L}(\theta,q) = \sum_{i=1}^N \sum_{c=1}^2 q(z_i=c) \log   \frac{p(x_i,z_i=c\mid \theta)}{q(z_i=c)}) }\]

<p>So, we do the above part in two steps as follows (Expectation step)</p>

\[\boxed{q^{k+1} = \text{argmax}_q \mathcal{L}(\theta^k,q)}\]

<p>and next (Maximization step)</p>

\[\boxed{\theta^{k+1} = \text{argmax}_\theta \mathcal{L}(\theta,q^{k+1})}\]

<h3 id="e-step">E-Step:</h3>

<p>Let’s look at the E-Step. In order to do so, let’s look at the difference between the log-likelihood and the lowerbound that we defined</p>

\[\log p(X,\theta)-\mathcal{L}(\theta,q) = \sum_{i=1}^N \log p(x_i\mid \theta)  - \sum_{i=1}^N \sum_{c=1}^2 q(z_i=c) \log   \frac{p(x_i,z_i=c\mid \theta)}{q(z_i=c)}) \\ = \sum_{i=1}^N \log p(x_i\mid \theta) \sum_{c=1}^2 q(z_i=c)  - \sum_{i=1}^N \sum_{c=1}^2 q(z_i=c) \log   \frac{p(x_i,z_i=c\mid \theta)}{q(z_i=c)}) \\ = \sum_{i=1}^N \sum_{c=1}^2 q(z_i=c)  \left( \log p(x_i\mid \theta)  -\log   \frac{p(x_i,z_i=c\mid \theta)}{q(z_i=c)}) \right)\\ = \sum_{i=1}^N \sum_{c=1}^2 q(z_i=c)  \left( \log   \frac{q(z_i=c) p(x_i\mid \theta) }{p(x_i,z_i=c\mid \theta)}) \right) \\ = \sum_{i=1}^N \sum_{c=1}^2 q(z_i=c)  \left( \log   \frac{q(z_i=c) }{p(z_i=c\mid x_i,\theta)}) \right) \\ = D_{KL}\left({q(z_i=c) }\mid \mid {p(z_i=c\mid x_i,\theta)} \right)\]

<p>So we found that</p>

\[\boxed{\log p(X\mid \theta) - \mathcal{L}(\theta,q) = D_{KL}\left({q(z_i=c) }\mid \mid {p(z_i=c\mid x_i,\theta)} \right)}\]

<p>which basically means that to maximize lowerbound \(\mathcal{L}(\theta,q)\) (which minimizes the distance between the log-likelihood and the lowerbound), we need to minimize the KL-divergence on the righthandside. The KL divergence is zero when the two PDFs are the same, so</p>

\[q(z_i=c) = p(z_i=c\mid x_i,\theta)\]

<h3 id="m-step">M-Step:</h3>

<p>Now we are interested to maximize the following with respect to \(\theta\) as</p>

\[\mathcal{L}(\theta,q) = \sum_{i=1}^N \sum_{c=1}^2 q(z_i=c) \log   \frac{p(x_i,z_i=c\mid \theta)}{q(z_i=c)})  \\= \sum_{i=1}^N \sum_{c=1}^2 q(z_i=c) \log   {p(x_i,z_i=c\mid \theta)}) - \sum_{i=1}^N \sum_{c=1}^2 q(z_i=c) \log   {q(z_i=c)})\\ = \mathbf{E}_q \log p(X,Z\mid \theta)\]

<h3 id="summary-of-expectation-maximization">Summary of Expectation-Maximization:</h3>

<p>E-step:</p>

\[\boxed{q^{k+1}(z_i) = p(z_i\mid x_i,\theta^k) }\]

<p>which results in the fact that \(\log p(X,\theta) = \mathcal{L}(\theta,q^{k+1})\). Next for the M-step, we have</p>

\[\boxed{\theta^{k+1} = \text{argmax}_\theta \mathbf{E}_q \log p(X,Z\mid \theta)}\]

<p>Note that this maximizes the lower bound, however it is guaranteed that \(\log p(X\mid \theta^{k+1}) \geq \mathcal{L}(\theta^{k+1},q^{k+1})\)</p>

<h3 id="convergence">Convergence</h3>

<p>We have</p>

\[\log p(X\mid \theta^{k+1}) \geq \mathcal{L}(\theta^{k+1},q^{k+1}) \geq \mathcal{L}(\theta^k,q^{k+1}) = \log p(X\mid \theta^k)\]

<h3 id="gaussian-mixture-models-gmm">Gaussian Mixture Models (GMM)</h3>

<p>In these models, we assume that the data is coming from a mixture of Gaussian distributions \(\mathcal{N}(\mu,\Sigma)\) and the latent distribution is a categorical distribution \(\phi\). This basically means that</p>

\[\boxed{p(X) = \sum_{k=1}^K \pi_k \mathcal{N}(X\mid \mu_k,\Sigma_k)}\]

<p>where \(\sum_k \pi_k = 1\). Note that \(\theta = \{ \mu_1,\Sigma_1, \cdots, \mu_K,\Sigma_K\}.\) Now, let’s assume the latent variable is called \(z_i\). Using Expectation-Maximization that we discussed here, we find that</p>

\[q(z_i = k) = p(z_i=k\mid x_i,\theta) = \frac{p(z_i=k) p(x_i \mid  z_i=k,\theta) }{p(x_i\mid \theta)} = \frac{p(z_i=k)p(x_i \mid  z_i=k,\theta) }{\sum_{k} p(z_i=k) p(x_i \mid  z_i=k,\theta) } \\ \boxed{q(z_i=k) = \frac{\pi_k \mathcal{N}(x_i\mid \mu_k,\Sigma_k)}{\sum_k\pi_k \mathcal{N}(x_i\mid \mu_k,\Sigma_k)}}\]

<p>Next, in the Maximization step, we have</p>

\[\theta^*= \max_\theta \mathbf{E}_q \log p(X,Z\mid \theta) = \max_\theta \log p(X\mid \theta) \\ \theta^* = \max_\theta \sum_i \log \left( \sum_k \pi_k \mathcal{N}(x_i\mid \mu_k,\sigma_k) \right)\]

<p>Taking the derivative with respect to the \(\mu_k\), we find that</p>

\[\frac{\partial \cdots}{\partial \mu_k} = 0 = \sum_i \frac{\pi_k \mathcal{N}(x_i\mid \mu_k,\sigma_k)}{\sum_k \pi_k \mathcal{N}(x_i\mid \mu_k,\sigma_k) } \\  \sum_i q(z_i=k)   \Sigma_k^{-1}(x_i - \mu_k) =0 =  \sum_i q(z_i=k)   (x_i - \mu_k) \\ \boxed{\mu_k =\frac{\sum_i q(z_i=k) x_i}{\sum_i q(z_i=k)}}\]

<p>Similarly, we can find that</p>

\[\boxed{\Sigma_k =   \frac{ \sum_i q(z_i=k)(x_i-\mu)^\top(x_i-\mu)}{  \sum_i q(z_i=k)}  }\]

<h3 id="k-means-as-em">K-Means as EM</h3>

<p>Imagine the K-means model, where we randomly initialize \(\theta = \{\mu_1, \cdots, \mu_C\}\) points, and then we repeat the following steps until convergence</p>

<ul>
  <li>For each point we calculate the closest centroid</li>
</ul>

\[z_i = \text{argmin}_k \\mid x_i-\mu_k\\mid ^2\]

<ul>
  <li>Update centroid</li>
</ul>

\[\mu_k = \frac{\sum_{i:z_i=k} x_i}{\sum_{i:z_i=k} 1}\]

<p>The above K-means model can be think of as a GMM. Imagine we fix the covariance matrix to be identity, $\Sigma_k = I$, and also we fix the weights to be \(\pi_k = 1/\#\text{of Gaussians}\). We then will have</p>

\[p(x_i\mid z_i=k,\theta) = \frac{1}{Z} \exp\left(-\frac{1}{2} \\mid x_i-\mu_k\\mid ^2\right)\]

<p>Then, in the E-step, we pick \(q(z)\) such that they belong to the delta functions. Then are interested to find a function from the family of delta functions such that</p>

\[q(z_i) = \begin{cases} 1 &amp; \text{ if } z_i = \text{argmax}_k p(z_i=k\mid x_i,\theta)\\ 0 &amp; \text{otherwise}\end{cases}\]

<p>Note that</p>

\[p(z_i=k\mid x_i,\theta) = \frac{1}{Z} p(x_i\mid z_i,\theta) p(z_i\mid \theta) = \frac{1}{Z} \exp(-\frac{1}{2}\\mid  x_i-\mu_k\\mid ^2) \pi_k\]

<p>So the above maximization problem becomes</p>

\[q(z_i) = \begin{cases} 1 &amp; \text{ if } z_i = \text{argmin}_k  \\mid x_i-\mu_k\\mid ^2 \\ 0&amp; \text{otherwise}\end{cases}\]

<p>which is the same as the first step in the K-means. Now, lets look at the M-step, using the GMM derivation that we did above, we ha</p>

\[\mu_k =\frac{\sum_i q(z_i=k) x_i}{\sum_i q(z_i=k)} = \frac{\sum_{i:z_i=k} x_i}{\sum_{i:z_i=k} 1}\]

<h3 id="implementing-gmm-in-python">Implementing GMM in python</h3>

<p>So in GMM we are implementing the following formulae</p>

<p>E-step:</p>

\[q(z_i=k) = \frac{\pi_k \mathcal{N}(x_i\mid \mu_k,\Sigma_k)}{\sum_k\pi_k \mathcal{N}(x_i\mid \mu_k,\Sigma_k)}\]

<p>M-Step:</p>

\[\mu_k =\frac{\sum_i q(z_i=k) x_i}{\sum_i q(z_i=k)}\]

\[\Sigma_k =   \frac{ \sum_i q(z_i=k)(x_i-\mu)^\top(x_i-\mu)}{  \sum_i q(z_i=k)}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="c1"># here we denote q(z_i=k) with a NxC matrix called gamma
# where N is the number of poitns i=1,...,N
# and C is the number of clusters
</span><span class="k">def</span> <span class="nf">E_step</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">pi</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="s">"""
    Performs E-step on GMM model
    Each input is numpy array:
    X: (N x d), data points
    pi: (C), mixture component weights
    mu: (C x d), mixture component means
    sigma: (C x d x d), mixture component covariance matrices

    Returns:
    gamma: (N x C), probabilities of clusters for objects

    P(z\mid x) = (p(x\mid z) p(z))/(sum_z p(x\mid z) p(z) )
    gamma  =
    """</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># number of objects
</span>    <span class="n">C</span> <span class="o">=</span> <span class="n">pi</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># number of clusters
</span>    <span class="n">d</span> <span class="o">=</span> <span class="n">mu</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># dimension of each object
</span>    <span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span> <span class="c1"># distribution q(T)
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
      <span class="n">pi_i</span> <span class="o">=</span> <span class="n">pi</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
      <span class="n">gamma</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">pi_i</span><span class="o">*</span><span class="n">stats</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">,:],</span><span class="n">sigma</span><span class="p">[</span><span class="n">i</span><span class="p">,:,:]).</span><span class="n">pdf</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">gamma</span>

<span class="k">def</span> <span class="nf">M_step</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
    <span class="s">"""
    Performs M-step on GMM model
    Each input is numpy array:
    X: (N x d), data points
    gamma: (N x C), distribution q(T)

    Returns:
    pi: (C)
    mu: (C x d)
    sigma: (C x d x d)
    """</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># number of objects
</span>    <span class="n">C</span> <span class="o">=</span> <span class="n">gamma</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># number of clusters
</span>    <span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># dimension of each object
</span>
    <span class="n">resp_weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">pi</span> <span class="o">=</span> <span class="n">resp_weights</span><span class="o">/</span><span class="p">(</span><span class="mf">1.0</span><span class="o">*</span><span class="n">N</span><span class="p">)</span>

    <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">gamma</span><span class="p">.</span><span class="n">T</span><span class="p">,</span><span class="n">X</span><span class="p">)</span><span class="o">/</span><span class="n">resp_weights</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">C</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">d</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
      <span class="n">diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="o">-</span><span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
      <span class="n">weighted_sum</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">((</span><span class="n">gamma</span><span class="p">[:,</span><span class="n">i</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">diff</span><span class="p">).</span><span class="n">T</span><span class="p">,</span><span class="n">diff</span><span class="p">)</span>
      <span class="n">sigma</span><span class="p">[</span><span class="n">i</span><span class="p">,:,:]</span> <span class="o">=</span> <span class="n">weighted_sum</span><span class="o">/</span><span class="n">resp_weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">pi</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span>

<span class="k">def</span> <span class="nf">train_EM</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">restarts</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="s">'''
    Starts with random initialization *restarts* times
    Runs optimization until saturation with *rtol* reached
    or *max_iter* iterations were made.

    X: (N, d), data points
    C: int, number of clusters
    '''</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># number of objects
</span>    <span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># dimension of each object
</span>    <span class="n">best_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span>
    <span class="n">best_pi</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="n">best_mu</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="n">best_sigma</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">restarts</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">pi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">C</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
            <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">C</span><span class="p">,</span><span class="n">d</span><span class="p">)</span>
            <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">C</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">d</span><span class="p">))</span>
            <span class="n">sigma</span><span class="p">[...]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">identity</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
            <span class="n">prev_loss</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
              <span class="n">gamma</span> <span class="o">=</span> <span class="n">E_step</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">pi</span><span class="p">,</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">)</span>
              <span class="n">pi</span><span class="p">,</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">M_step</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">gamma</span><span class="p">)</span>
              <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_vlb</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">pi</span><span class="p">,</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">,</span><span class="n">gamma</span><span class="p">)</span>

              <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="p">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="ow">and</span> <span class="n">loss</span><span class="o">&gt;</span><span class="n">best_loss</span><span class="p">:</span>
                <span class="n">best_loss</span> <span class="o">=</span> <span class="n">loss</span>
                <span class="n">best_mu</span> <span class="o">=</span> <span class="n">mu</span>
                <span class="n">best_pi</span> <span class="o">=</span> <span class="n">pi</span>
                <span class="n">best_sigma</span> <span class="o">=</span> <span class="n">sigma</span>

              <span class="k">if</span> <span class="n">prev_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">prev_loss</span><span class="o">-</span><span class="n">loss</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">prev_loss</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">diff</span> <span class="o">&lt;</span> <span class="n">rtol</span><span class="p">:</span>
                  <span class="k">break</span>
              <span class="n">prev_loss</span> <span class="o">=</span> <span class="n">loss</span>

        <span class="k">except</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">LinAlgError</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Singular matrix: components collapsed"</span><span class="p">)</span>
            <span class="k">pass</span>

    <span class="k">return</span> <span class="n">best_loss</span><span class="p">,</span> <span class="n">best_pi</span><span class="p">,</span> <span class="n">best_mu</span><span class="p">,</span> <span class="n">best_sigma</span>
</code></pre></div></div>

<p>Benefits of GMM models: in unsupervised clustering for example, KNN methods accuracy will increase as we increase the number of groups. So you never know how many groups are better for KNN methods. In GNN on the other hand, the accuracy increases and then decreases. So increasing the number of clusters does not necessary increase the accuracy.</p>

<h3 id="dirichlet-distribution">Dirichlet Distribution</h3>

<p>A Dirichlet distribution is defined as</p>

\[f(\theta;\alpha) = \frac{1}{B(\alpha)} \Pi_{i=1}^K \theta_i^{\alpha_i-1}\]

<p>Note that \(\sum_{i=1}^K \theta_i = 1\) and \(\theta_i&gt;0\). The expected value and variance can be found as</p>

<p>Note that the Dirichlet distribution is conjugate to the multinomial distribution as</p>

\[p(\theta) = \frac{n!}{x_1! \cdots x_K!} \Pi_{i=1}^K \theta_i^{x_i}\]

<p>So if prior has a Dirichlet distribution, and likelihood is a multinomial, then the posetrior will also be a Dirichlet distribution.</p>

<h3 id="latent-dirichelet-model">Latent Dirichelet Model</h3>

<p>Imagine the following model for the distribution of words in a document</p>

<p><br /></p>
<div class="row mt-3" style="text-align:center;">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" width="300" src="/assets/research/bayes1.jpg" />
    </div>
</div>
<p><br /></p>

<p>In latent Dirichelet model, we have</p>

\[p(W,Z,\theta) = \Pi_{d=1}^D p(\theta_d) \Pi_{n=1}^{N_d} p(z_{dn}\mid \theta_d)~p(w_{dn}\mid z_{dn})\]

<p>where \(p(\theta_d)\sim \text{Dir}(\alpha)\), and \(p(z_{dn}\mid \theta_d) = \theta_{dz_{dn}}\), and \(p(w_{dn}\mid z_{dn}) = \Phi_{z_{dn},w_{dn}}\), where \(\sum \Phi_{z_{dn},w_{dn}} =1\). In order to calculate the E-step, we first need to find the log-likelihood, where we have</p>

\[\log p(W,Z,\theta) = \sum_{d=1}^D \left[ \sum_{t=1}^T (\alpha_t-1) \log \theta_{dt} +  \sum_{n=1}^{N_d} \sum_{t=1}^T \mathbf{1}(z_{dn}=t) \left(\log \theta_{dt} + \log \phi_{tw_{dn}} \right) \right] + \text{C.}\]

<p>In the E-step, we want to</p>

\[\min D_{KL}\left( q(\theta) q(Z) \\mid  p(\theta,Z\mid W) \right)\]

\[\log q(\theta) = \mathbf{E}_{q(z)} \log p(\theta,Z,W)  =  \mathbf{E}_{q(z)} \log p(\theta,Z\mid  W) + C.\]

\[\log q(\theta) = \mathbf{E}_{q(z)} \sum_{d=1}^D \left[ \sum_{t=1}^T (\alpha_t-1) \log \theta_{dt} +  \sum_{n=1}^{N_d} \sum_{t=1}^T\mathbf{1}(z_{dn}=t) \left(\log \theta_{dt} + \log \phi_{tw_{dn}} \right) \right]\\ = \mathbf{E}_{q(z)} \sum_{d=1}^D \left[ \sum_{t=1}^T (\alpha_t-1) \log \theta_{dt} +  \sum_{n=1}^{N_d}\sum_{t=1}^T \mathbf{1}(z_{dn}=t) \left(\log \theta_{dt} \right) \right] \\ =  \sum_{d=1}^D \left[ \sum_{t=1}^T (\alpha_t-1) \log \theta_{dt} +  \sum_{n=1}^{N_d} \sum_{t=1}^T \mathbf{E}_{q(z)}\left[\mathbf{1}(z_{dn}=t)\right] \left(\log \theta_{dt}  \right) \right] \\  =  \sum_{d=1}^D \sum_{t=1}^T \log \theta_{dt} \left[ (\alpha_t-1)  +  \sum_{n=1}^{N_d} \mathbf{E}_{q(z)}\left[\mathbf{1}(z_{dn}=t)\right] \right]\]

<p>So in summary, we have</p>

\[\boxed{\log q(\theta) =  \sum_{d=1}^D \sum_{t=1}^T \log \theta_{dt} \left[ (\alpha_t-1)  +  \sum_{n=1}^{N_d} \gamma_{dn}^t \right]} \\ \boxed{\gamma_{dn}^t = \mathbf{E}_{q(z)}\left[\mathbf{1}(z_{dn}=t)\right]}\]

\[\to q(\theta) \propto \Pi_d \Pi_t \theta_{dt}^{\left[\alpha_t + \sum_n \gamma^t_{dn} - 1\right]}\]

<p>Now, let’s take the E-step for \(q(Z)\), we have</p>

\[\log q(Z) = \\  \mathbf{E}_{q(\theta)} \sum_{d=1}^D \left[ \sum_{t=1}^T (\alpha_t-1) \log \theta_{dt} +  \sum_{n=1}^{N_d} \sum_{t=1}^T\mathbf{1}(z_{dn}=t) \left(\log \theta_{dt} + \log \phi_{tw_{dn}} \right) \right] \\ = \mathbf{E}_{q(\theta)} \sum_{d=1}^D  \sum_{n=1}^{N_d} \mathbf{1}(z_{dn}=t) \left(\log \theta_{dt} + \log \phi_{tw_{dn}} \right)  \\ =  \sum_{d=1}^D  \sum_{n=1}^{N_d} \mathbf{1}(z_{dn}=t) \left(\mathbf{E}_{q(\theta)} \log \theta_{dt} + \log \phi_{tw_{dn}} \right)\]

<p>so in summary</p>

\[\boxed{\log q(Z) = \sum_{d=1}^D  \sum_{n=1}^{N_d} \mathbf{1}(z_{dn}=t) \left(\mathbf{E}_{q(\theta)} \log \theta_{dt} + \log \phi_{tw_{dn}} \right) } \\ \to q(Z) = \Pi_d \Pi_t q(z_{dn})\\ q\left( z_{dn}=t\right) =   \frac{\phi_{t w_{dn}} \exp \left( \mathbf{E}_{q(\theta)} \log \theta_{dt} \right)}{\sum_{t'} {\phi_{t' w_{dn}} \exp \left( \mathbf{E}_{q(\theta)} \log \theta_{dt'} \right)}}\]

<p>and in the M-step we would like to maximize the following</p>

\[\mathbf{E}_{q(\theta)q(Z)} \log p(\theta,Z,W)  = \mathbf{E}_{q(\theta)q(Z)}  \left[ \sum_{d=1}^D \left[ \sum_{t=1}^T (\alpha_t-1) \log \theta_{dt} +  \sum_{n=1}^{N_d} \sum_{t=1}^T \mathbf{1}(z_{dn}=t) \left(\log \theta_{dt} + \log \phi_{tw_{dn}} \right) \right] \right] \\ = \mathbf{E}_{q(\theta)q(Z)}  \left[ \sum_{d=1}^D \sum_{t=1}^T \sum_{n=1}^{N_d} \mathbf{1}(z_{dn}=t)  \log \phi_{tw_{dn}} \right] \\  =   \sum_{d=1}^D \sum_{t=1}^T \sum_{n=1}^{N_d} \mathbf{E}_{q(\theta)q(Z)} \left[\mathbf{1}(z_{dn}=t)\right]  \log \phi_{tw_{dn}} \\ =   \sum_{d=1}^D \sum_{t=1}^T \sum_{n=1}^{N_d} \gamma_{dn}^t  \log \phi_{tw_{dn}}\]

<p>given that \(\sum_w \phi_{tw} = 1\) and also \(\phi_{tw}\geq 0\). We use lagrangian to maximize the above equation, we have</p>

\[L =  \sum_{d=1}^D \sum_{t=1}^T \sum_{n=1}^{N_d} \gamma_{dn}^t \log \phi_{tw_{dn}} - \sum_{t=1}^T \lambda_t \left(\sum_w \phi_{tw}-1\right)\]

<p>Now we take the derivative to maximize the above equation, we find</p>

\[\frac{\partial L}{\partial \phi_{tw}} = \sum_{d=1}^D \sum_{n=1}^{N_d} \gamma_{dn}^t \frac{1}{\phi_{tw}}\mathbf{1}\left[w_{dn} = w\right] -  \lambda_t  = 0 \\ \phi_{tw} = \frac{\sum_{d=1}^D \sum_{n=1}^{N_d} \gamma_{dn}^t \mathbf{1}\left[w_{dn}= w\right]}{ \lambda_t}\]

<p>Knowing that \(\sum_w \phi_{tw} = 1\), we can find that</p>

\[\phi_{tw} = \frac{\sum_{d=1}^D \sum_{n=1}^{N_d} \gamma_{dn}^t \mathbf{1}\left[w_{dn}= w\right]}{ \sum_w \sum_{d=1}^D \sum_{n=1}^{N_d} \gamma_{dn}^t \mathbf{1}\left[w_{dn}= w\right]} \\ \to \boxed{\phi_{tw} = \frac{\sum_{d,n} \gamma_{dn}^t \mathbf{1}\left[w_{dn}= w\right]}{ \sum_{d,n,w'} \gamma_{dn}^t \mathbf{1}\left[w_{dn}= w'\right]}}\]

<h2 id="monte-carlo-method">Monte-Carlo Method</h2>

<p>Estimating expected values using sampling</p>

\[\mathbf{E}_{p(x)} f(x) = \frac{1}{M} \sum f(x_s), \quad \text{where } x_s\sim p(x)\]

<p>Now the question is how to sample from a probability distribution \(p(x)\)? In the following we will discuss this. Note that we assume that generating a random number with uniform distribution from in \([0,1]\) is given, i.e., we can easily sample form \(\mathcal{U}(0,1)\).</p>

<h3 id="sampling-from-1-d-distribution">Sampling from 1-D distribution</h3>

<p>Consider a distribution over discrete set such as \(p(a_i) = p_i\) for \(i=1,\cdots,n\). Note that \(\sum_i p_i = 1\). We can separate the \([0,1]\) distance proportional to the \(p_i\). Next, we sample a point from $[0,1]$ using the uniform distribution, we can assign it to the discrete values of \(a_i\) based on the interval that it lands into. Here is an example:</p>

<p><br /></p>
<div class="row mt-3" style="text-align:center;">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" width="300" src="/assets/research/bayes2.png" />
    </div>
</div>
<p><br /></p>

<p>Sampling Normal distribution:</p>

<p>We can generate normal distribution using central limit theorem, i.e.</p>

\[x = \frac{1}{\sqrt{N}} \left[ \sum_{i=1}^N \left( x_i - \frac{1}{2}\right) \right]\]

<p>As the \(N\to \infty\), the \(p(x) \to \mathcal{N}(0,1)\). This has been done very efficiently, and we can use packages such as <code class="language-plaintext highlighter-rouge">np.random.randn()</code> to generate these numbers. Now imagine that we are interested in 1d sampling from a continuous distribution such as \(p(x)\) shown below</p>

<p><br /></p>
<div class="row mt-3" style="text-align:center;">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" width="300" src="/assets/research/bayes3.png" />
    </div>
</div>
<p><br /></p>

<p>One way to sample from \(p(x)\) in the above is to first bound the pdf by some normal distribution $q(x)$. Next, we generate a random point, say \(x_0\). We then accept this point with probability \(\alpha= p(x)/q(x)\) and reject it with \(1-\alpha\). This way, we create samples from the \(p(x)\).</p>

<h3 id="markov-chains">Markov-Chains</h3>

<p>There are two methods that we will introduce here (Metropolis-Hasting and Gibbs sampling) that depend on Markov-Chain. A few introductory remarks on Markov-process is helpful before we talk about them.</p>

<p>Let \(X_t\) denote the value of a random variable at time $t$. The <em>state space</em> is the space of all possible values for $X$ values. The random variable is called a <strong>Markov process</strong> if the transition probabilities betwen different values in the state space only depend on the current’s value of the random variable, i.e.,</p>

\[p(X_{t+1}=s_{j}\mid X_0=s_k, \cdots, X_t=s_i) = p(X_{t+1}=s_{j}\mid  X_t=s_i)\]

<p>A Markov-Chain referes to a sequence of random variables \((X_0, \cdots,X_n)\) generated by a markov process. Transition probability (or the transition kernel) gives us the probability of transitioning between the states ina single step, i.e.,</p>

\[p(i\to j) = p(X_{t+1}=s_j\mid  X_{t}=s_i)\]

<p>Using the transition kernel, if we are at state \(s_i\) we can define a row vector for the state space probabilities, i.e., \(\pi_i(t) = p(X_t = s_i)\). The evolution of this state space probabilities can be obtained using the kernel as</p>

\[\pi_j(t+1) = p(X_{t+1}=s_j) \\= \sum_k p(X_{t+1}=s_j\mid   X_t = s_k) p(X_t = s_k) = \sum_k p(k,j) \pi_k(t) \\ \mathbf{\pi}(t+1) = \mathbf{\pi}(t) \mathbf{P}\]

<p>As a result, we can find that \(\pi(t) = \pi(0) \mathbf{P}^k\). A distribution of states $\mathbf{\pi}^*$ is called stationary if \(\mathbf{\pi}^* = \mathbf{\pi}^*\mathbf{P}\). A sufficient condition for a unique stationary distribution is that the <strong>detailed balance</strong> condition holds</p>

\[p(j\to k) \pi^*_j = p(k\to j) \pi^*_k\]

<p>If the above condition holds then we have</p>

\[(\mathbf{\pi}\mathbf{P})_j = \sum_i \pi_i P(i,j) = \sum_i \pi_i P(i\to j) \\= \sum_i \pi_j P(j\to i)  = \pi_j \sum_i P(j\to i) = \pi_j\]

<h3 id="metropolis-hasting-algorithm">Metropolis-Hasting Algorithm</h3>

<p>So our goal is to generate a sample with PDF \(p(x)\). In Metropolis-Hasting the basic idea is to create a Markov-Process to generate new data points where \(p(x)\) is its stationary distribution. If \(p(x)\) is stationary distribution, then using following the Markov process for a long time we will generate data-points that have the distribution of \(p(x)\). But how can we create a Markov-Process where \(p(x)\) is its stationary distribution? We can use the idea of rejecting points. We start with any Markov-process \(Q\) (as long as we have a non-zero probability of going over all the data points), we then start from a data-point \(x_0\) (or state) and find our new data-point \(x_1\) (the new state). Now we have an option of accepting this new state or rejecting it. We select this acceptance/rejection probability such that our \(p(x)\) becomes the stationary distribution of our Markov process.</p>

\[p(x_0) Q(x_0\to x_1) A(x_0 \to x_1) = p(x_1) Q(x_1\to x_0) A(x_1 \to x_0) \\ \frac{A(x_0 \to x_1)}{A(x_1 \to x_0)} = \frac{p(x_1)}{p(x_0)} \frac{Q(x_1\to x_0)}{Q(x_0\to x_1)} = \rho\]

<p>So as long as our acceptance probability follows the above equality, we are doing well. Assume that the above proportionality is \(\rho\). If \(\rho\leq 1\), we can have \(A(x_0\to x_1) = \rho, A(x_1\to x_0) = 1\). On the other hand, if \(\rho&gt;1\), we can assign \(A(x_0\to x_1) = 1, A(x_1\to x_0) = 1/\rho\). So basically, we can assign the following acceptance probability</p>

\[A(x_0\to x_1) = \begin{cases} \rho &amp; \rho \leq 1 \\ 1 &amp; \rho &gt;1 \end{cases}\]

<p>Or we can summarize it as</p>

\[A(x_0\to x_1) = \min\left(1, \frac{p(x_1)}{p(x_0)} \frac{Q(x_1\to x_0)}{Q(x_0\to x_1)} \right)\]]]></content><author><name></name></author><summary type="html"><![CDATA[KL Divergence]]></summary></entry><entry><title type="html">Kalman Filter</title><link href="https://azareei.github.io/blog/2021/Kalman-Filter/" rel="alternate" type="text/html" title="Kalman Filter" /><published>2021-03-02T08:10:00+10:00</published><updated>2021-03-02T08:10:00+10:00</updated><id>https://azareei.github.io/blog/2021/Kalman-Filter</id><content type="html" xml:base="https://azareei.github.io/blog/2021/Kalman-Filter/"><![CDATA[<p>Kalman filter is a classic state estimation technique that has found application in many places. In this simple tutorial, I will try to explain Kalman filter in an intuitive way. This is the most basic introduction to the Kalman filter and basically how I learned it. Before getting to the Kalman filter, I will first review some basic materials that we need.</p>

<p><br /></p>
<h1 id="prerequisite">Prerequisite</h1>

<p>Let \(x_i\) be a random variable that has a <strong>probability density function</strong> \(p_i(x)\) whose mean and variance are \(\mu_i\) and \(\sigma_i^2\). We write \(x_i \sim p_i(\mu_i,\sigma_i^2)\).</p>

<p>Assuming a set of pairwise uncorrelated random variables \(x_1 \sim p_1(\mu_1,\sigma_1^2), \cdots x_n \sim p_n(\mu_n,\sigma_n^2)\), if \(y\) is a random variable where \(y = \sum_{i=1}^n \alpha_i x_i\), then the mean and variance of \(y\) are</p>

\[\mu_y = \sum_{i=1}^n \alpha_i \mu_i\]

\[\sigma_y^2 = \sum_{i=1}^n \alpha_i \sigma_i^2\]

<p><br /></p>
<h2 id="fusing-two-variables">Fusing two variables</h2>

<p>Now, imagine that we want to measure a variable \(y\), we have two totally different devices where they use different methods, one is based on an old method for example and its results are reported with \(x_1 \sim p_1(\mu_1,\sigma_1^2)\), and one that uses a new method and its the results are reported with \(x_2 \sim p_2(\mu_2,\sigma_2^2)\). Now the question is how to combine these two different measurements to create an optimal estimator for \(y\).  The simplest way is to combine these results linearly as \(y = \alpha x_1 + \beta x_2\).  A reasonable requirement is that if the two estimates \(x_1\) and \(x_2\) are giving the same result, then this linear combination should give out that same result. This implies that \(\alpha + \beta =1\). So our linear estimator so far becomes</p>

\[y_\alpha(x_1,x_2) = \alpha x_1 + (1-\alpha)x_2\]

<p>But what value should we pick for \(\alpha\)? One reasonable way is to say that the optimal value of \(\alpha\) minimizes the variance of \(y_\alpha\). The variance of \(y_\alpha\)  is</p>

\[\sigma_y^2 = \alpha^2 \sigma_1^2 + (1-\alpha)^2 \sigma_2^2\]

\[\frac{d}{d \alpha} \sigma_y^2 = 2\alpha \sigma_1^2 -2 (1-\alpha)\sigma_2^2 = 0 \to \alpha = \frac{\sigma_2^2}{\sigma_1^2 + \sigma_2^2}\]

<p>Since the second derivative is positive then this value of \(\alpha\) minimizes the variance. The estimator then becomes</p>

\[y(x_1,x_2) = \frac{\sigma_2^2}{\sigma_1^2 + \sigma_2^2} x_1 + \frac{\sigma_1^2}{\sigma_1^2 + \sigma_2^2} x_2\]

\[y(x_1,x_2) = \frac{1/\sigma_1^2}{1/\sigma_1^2 + 1/\sigma_2^2} x_1 + \frac{1/\sigma_2^2}{1/\sigma_1^2 + 1/\sigma_2^2} x_2, \quad \sigma_y^2 = \frac{1}{1/\sigma_2^1 + 1/\sigma_2^2}\]

<p><br /></p>
<h2 id="fusing-multiple-variables">Fusing multiple variables</h2>

<p>The above argument can be extended for multiple scalar estimates. Let \(x_i \sim p_i(\mu_i,\sigma_i^2)\) be a set of pairwise uncorrelated random variables. Consider unbiased linear estimator \(y = \sum_{i=1}^n \alpha_i x_i\).  Using Lagrange multipliers, we have</p>

\[f(\alpha_1, \cdots, \alpha_n) = \sum_{i=1}^n \alpha_i^2 \sigma_i^2 + \lambda \left( \sum_{i=1}^n \alpha_i -1 \right)\]

<p>where \(\lambda\)  is the Lagrange multiplier. Taking the derivative with respect to \(\alpha_j\) we find that \(\alpha_1 \sigma_1^2 = \alpha_2\sigma_2^2 = \cdots = -\lambda/2\). Since \(\sum \alpha_i = 1\), then we can find that</p>

\[\alpha_i = \frac{\frac{1}{\sigma_i^2}}{\sum_{i=1}^n \frac{1}{\sigma_i^2}}\]

<p>where the variance \(\sigma_y\)  is</p>

\[\sigma_y = \frac{1}{\sum_{i=1}^n \frac{1}{\sigma_i^2}}\]

<p><br /></p>
<h2 id="vector-estimates">Vector estimates</h2>

<p>Now let’s expand the same result to the vectors of random variables. Let \(\mathbf{x}_1 \sim p_1( \mathbf{\mu}_1,\Sigma_1), \cdots, \mathbf{x}_n \sim p_n( \mathbf{\mu}_n,\Sigma_n)\) be a set of pairwise uncorrelated random variables of length \(m\). If random variable \(\mathbf{y}\) is a linear combination of these random variables as \(\mathbf{y}  = \sum_{i=1}^n \mathbf{A}_i \mathbf{x}_i\), then the mean and covariance of \(\mathbf{y}\) is obtianed as</p>

\[\mathbf{\mu}_\mathbf{y} = \sum_{i=1}^n \mathbf{A}_i \mathbf{\mu}_i\]

\[\Sigma_\mathbf{yy} = \sum_{i=1}^n \mathbf{A}_i \Sigma_i\mathbf{A}^\top_i\]

<p><br /></p>
<h2 id="fusing-multiple-vector-estimates">Fusing multiple vector estimates</h2>

<p>Imagine the linear estimator as</p>

\[\mathbf{y}(\mathbf{x}_1,\cdots,\mathbf{x}_n) = \sum_{i=1}^n \mathbf{A}_i \mathbf{x}_i, \quad \sum \mathbf{A}_i = \mathbb{I}\]

<p>Similarly, we intend to minimize \(\mathbb{E}[ (\mathbf{y}-\mu)^\top (\mathbf{y}-\mu)]\) . We define the following optimization problem using Lagrangian multipliers</p>

\[f(\mathbf{A}_1, \cdots, \mathbf{A}_n) = \mathbb{E} \left[\sum_{i=1}^n (\mathbf{x}_i-\mu_i)^\top \mathbf{A}^\top_i \mathbf{A}_i (\mathbf{x}_i - \mu_i) \right] + \langle \Lambda, \mathbf{A}_i-\mathbb{I}\rangle\]

<p>where the second term is the Lagrangian multipliers and \(\langle \Lambda, \mathbf{A}_i-\mathbb{I}\rangle = \text{tr}\left[\Lambda^\top\left(  \mathbf{A}_i-\mathbb{I}\right)\right]\). Taking derivative of \(f\) with respect to \(\mathbf{A}_i\) and setting each derivative to zero to find the optimal values of \(\mathbf{A}_i\) gives us</p>

\[\mathbb{E} \left[2\mathbf{A}_i (\mathbf{x}_i-\mu_i) (\mathbf{x}_i - \mu_i)^\top  +  \Lambda \right]=0\]

\[2\mathbf{A}_i \Sigma_i + \Lambda = 0\to \mathbf{A}_1 \Sigma_1 = \mathbf{A}_2 \Sigma_2  = \cdots = \mathbf{A}_n \Sigma_n  = \frac{-\Lambda}{2}\]

<p>Using the fact that \(\sum \mathbf{A}_i = \mathbb{I}\),</p>

\[\mathbf{A}_i = \left( \sum_{i=1}^n \Sigma_j^{-1}\right)^{-1} \Sigma_i^{-1}\]

<p>Therefore the optimal estimator becomes</p>

\[\mathbf{y} =  \left( \sum_{i=1}^n \Sigma_j^{-1}\right)^{-1}\sum_{i=1}^n  \Sigma_i^{-1} \mathbf{x}_i, \qquad \Sigma_{\mathbf{y}\mathbf{y}} = \left( \sum_{i=1}^n \Sigma_j^{-1}\right)^{-1}\]

<p><br /></p>
<h2 id="special-case-of-n2">Special case of \(n=2\)</h2>

<p>Let \(\mathbf{x}_1 \sim p_1(\mu_1, \Sigma_1)\), and \(\mathbf{x}_2 \sim p_2 (\mu_2,\Sigma_2)\), then we have</p>

\[\mathbf{K} = \Sigma_1 \left(\Sigma_1 + \Sigma_2 \right)^{-1}\]

\[\mathbf{y} = \mathbf{x}_1 + \mathbf{K} (\mathbf{x}_2-\mathbf{x}_1), \quad \Sigma_{\mathbf{y}\mathbf{y}} = (\mathbf{I}-\mathbf{K})\Sigma_1\]

<p>In order to prove the above relation, we start from the relation we obtained above, i.e.</p>

\[\mathbf{y} = \left( \Sigma_1^{-1} + \Sigma_2^{-1} \right)^{-1} \left( \Sigma_1^{-1} \mathbf{x}_1+ \Sigma_2^{-1}\mathbf{x}_2 \right)\]

<p>Note that the following matrix identity holds true \((\mathbf{A} ^{-1} + \mathbf{B}^{-1})^{-1} = \mathbf{A} (\mathbf{A} + \mathbf{B})^{-1} \mathbf{B} = \mathbf{B} (\mathbf{A} + \mathbf{B})^{-1} \mathbf{A}\)</p>

\[\mathbf{y} = \Sigma_2  \left( \Sigma_1 + \Sigma_2 \right)^{-1} \Sigma_1 \Sigma_1^{-1} \mathbf{x}_1  + \Sigma_1 \left( \Sigma_1 + \Sigma_2 \right)^{-1} \Sigma_2  \Sigma_2^{-1}\mathbf{x}_2\]

\[\mathbf{y} = \Sigma_2  \left( \Sigma_1 + \Sigma_2 \right)^{-1}  \mathbf{x}_1  + \Sigma_1 \left( \Sigma_1 + \Sigma_2 \right)^{-1} \mathbf{x}_2\]

<p>We add and subtract \(\Sigma_1  \left( \Sigma_1 + \Sigma_2 \right)^{-1}  \mathbf{x}_1\)  to the above equation to obtain</p>

\[\mathbf{y} = \Sigma_2  \left( \Sigma_1 + \Sigma_2 \right)^{-1}  \mathbf{x}_1  + \Sigma_1 \left( \Sigma_1 + \Sigma_2 \right)^{-1} \mathbf{x}_2 + \Sigma_1  \left( \Sigma_1 + \Sigma_2 \right)^{-1}  \mathbf{x}_1  - \Sigma_1  \left( \Sigma_1 + \Sigma_2 \right)^{-1}  \mathbf{x}_1\]

\[\boxed{\mathbf{y} =  \mathbf{x}_1  + \Sigma_1 \left( \Sigma_1 + \Sigma_2 \right)^{-1} \left( \mathbf{x}_2 - \mathbf{x}_1\right) }\]

<p>Similarly for the covariance matrix we have</p>

\[\Sigma_{\mathbf{y}\mathbf{y}} = \left( \Sigma_1^{-1} + \Sigma_2^{-1} \right)^{-1} = \Sigma_1 \left( \Sigma_1^{-1} + \Sigma_2^{-1} \right)^{-1}  \Sigma_2\]

<p>We add and subtract the term \(\left( \Sigma_1^{-1} + \Sigma_2^{-1} \right)^{-1}  \Sigma_1\) to the above equatio to obtain</p>

\[\Sigma_{\mathbf{y}\mathbf{y}} = \Sigma_1 \left( \Sigma_1^{-1} + \Sigma_2^{-1} \right)^{-1}  \Sigma_2 + \Sigma_1\left( \Sigma_1^{-1} + \Sigma_2^{-1} \right)^{-1}  \Sigma_1 - \Sigma_1\left( \Sigma_1^{-1} + \Sigma_2^{-1} \right)^{-1}  \Sigma_1\]

\[\Sigma_{\mathbf{y}\mathbf{y}} = \Sigma_1  - \Sigma_1\left( \Sigma_1^{-1} + \Sigma_2^{-1} \right)^{-1}  \Sigma_1\]

\[\Sigma_{\mathbf{y}\mathbf{y}} = \left( \mathbf{I}  - \Sigma_1\left( \Sigma_1^{-1} + \Sigma_2^{-1} \right)^{-1}\right)  \Sigma_1 = \left( \mathbf{I}  - \mathbf{K} \right)  \Sigma_1\]

<p><br /></p>
<h2 id="best-linear-unbiased-estimator">Best Linear Unbiased Estimator</h2>

<p>Let \(\left( \begin{matrix} \mathbf{x} \\ \mathbf{y}\end{matrix}\right) \sim p\left(\left( \begin{matrix} \mu_\mathbf{x} \\ \mu_\mathbf{y} \end{matrix}\right), \left( \begin{matrix} \Sigma_\mathbf{xx}&amp; \Sigma_\mathbf{xy} \\ \Sigma_\mathbf{yx} &amp; \Sigma_\mathbf{yy}\end{matrix}\right) \right)\) . The estimator \(\hat{\mathbf{y}} = \mathbf{A}\mathbf{x} + \mathbf{b}\) for estimating values of \(\mathbf{y}\) for a given \(\mathbf{x}\) is</p>

\[\mathbf{A} = \Sigma_\mathbf{yx} \Sigma_\mathbf{xx}^{-1}\]

\[\mathbf{b} = \mu_\mathbf{y} - \mathbf{A}\mu_\mathbf{x}\]

<h1 id="kalman-filter-for-a-linear-system">Kalman Filter for a linear system</h1>

<p>Now that we know all the ingredients we can discuss the Kalman filter. Assume a linear dynamical system where</p>

\[\mathbf{x}_k = \mathbf{F}_k\mathbf{x}_{k-1} + \mathbf{B}_k \mathbf{u}_k  + \mathbf{w}_k\]

<p>where \(\mathbf{F}_k\) is the state transition model applied to the previous state \(\mathbf{x}_{k-1},\)  and \(\mathbf{B}_k\) is the control input model applied to the control vector \(\mathbf{u}_k\), and \(\mathbf{w}_k\) is the process noise assumed to be drawn from a multivariate normal distribution with \(\mathcal{N}(0,\mathbf{Q})\) where \(\mathbf{Q}\) is the covariance matrix. At time \(k\), we do an observation (or measurement) \(\mathbf{z}_k\) of the true state \(\mathbf{x}_k\) according to the</p>

\[\mathbf{x}_k = \mathbf{H}_k \mathbf{x}_k + \mathbf{v}_k\]

<p>where \(\mathbf{H}_k\) is the observation model, and \(\mathbf{v}_k\) is the observation noise drawn from Gaussian noise \(\mathcal{N}(0,\mathbf{R}_k)\) where \(\mathbf{R}_k\) is the covariance matrix.</p>

<p>First let’s assume that \(\mathbf{H}_k= \mathbf{I}\) where we fully observe the state. Given an estimate
that we have at time \(t-1\) based on all the observations we had as  \(\hat{\mathbf{x}}_{t-1|t-1}\), we make a prediction for \(\hat{\mathbf{x}}_{t|t-1}\) based on the dynamical system equation as</p>

\[\hat{\mathbf{x}}_{t|t-1} = \mathbf{F}_t\hat{\mathbf{x}}_{t-1|t-1} + \mathbf{B}_t \mathbf{u}_t\]

<p>Next the variance can also be estimated as</p>

\[\Sigma_{t|t-1} = \mathbf{F}_t \Sigma_{t-1|t-1}\mathbf{F}_t^\top + \mathbf{Q}_t\]

<p>Given these predictions for that state at time \(t\), we also make an observation as \(\mathbf{z}_t = \mathbf{x}_t\) where the covariance matrix is \(\mathbf{R}_t\).</p>

<p>Now our goal is to combine these results to correct our estimate of
\(\mathbf{x}_{t|t}\)
. We use the derivation that we did above to combine these results based on their covariance matrix such that the covariance is minimized, we have</p>

\[\boxed{\mathbf{K}_t= \Sigma_{t|t-1} \left( \Sigma_{t|t-1} + \mathbf{R}_t \right)^{-1}}\]

\[\boxed{\hat{\mathbf{x}}_{t|t} = \hat{\mathbf{x}}_{t|t-1} + \mathbf{K}_t \left( \mathbf{z}_t - \hat{\mathbf{x}}_{t|t-1}\right)}\]

\[\boxed{\Sigma_{t|t} = \left( \mathbf{I} - \mathbf{K}_t\right)\Sigma_{t|t-1}}\]

<p>Now let’s imagine what happens if we only do a partial observation of the state or \(\mathbf{H}_k\neq \mathbf{I}\). In this case, we do the prediction as before, but in the step that we want to combine the results to correct the prediction, we need to make some changes since we only have partial parts of \(\mathbf{x}_t\). In such a case, we used the best linear estimator that we introduced earlier to construct the full \(\mathbf{x}_t\) and then use that to update the prediction.</p>

<p>The estimation with partial observation becomes</p>

\[\mathbf{H}_t\hat{\mathbf{x}}_{t|t} = \mathbf{H}_t\hat{\mathbf{x}}_{t|t-1} + \mathbf{H}_t  \Sigma_{t|t-1} \left( \Sigma_{t|t-1} + \mathbf{R}_t \right)^{-1} \mathbf{H}_t^\top\left( \mathbf{z}_t - \mathbf{H}_t\hat{\mathbf{x}}_{t|t-1}\right)\]

<p>We can define</p>

\[\boxed{\mathbf{K}_t = \Sigma_{t|t-1} \left( \Sigma_{t|t-1} + \mathbf{R}_t \right)^{-1} \mathbf{H}_t^\top}\]

<p>and the observable simplifies to</p>

\[\mathbf{H}_t\hat{\mathbf{x}}_{t|t} = \mathbf{H}_t\hat{\mathbf{x}}_{t|t-1} + \mathbf{H}_t \mathbf{K}_t\left( \mathbf{z}_t - \mathbf{H}_t\hat{\mathbf{x}}_{t|t-1}\right)\]

<p>The rest of the variables (hidden states) can be obtained using
\(\mathbf{C}_t\hat{\mathbf{x}}_{t|t-1}\)
where
\(\left(\begin{matrix} \mathbf{H}_t \\ \mathbf{C}_t\end{matrix} \right)\)
becomes an invertible matrix. The simplest example is to have it be equal to the identity matrix. The covariance between
\(\mathbf{C}_t\hat{\mathbf{x}}_{t|t-1}\)
and the observable
\(\mathbf{H}_t\hat{\mathbf{x}}_{t|t-1}\)
is
\(\mathbf{C}_t \Sigma_{t|t-1} \mathbf{H}_t^\top\)
. Using the best linear estimate estimator, we can find the hidden portion estimation as</p>

\[\mathbf{C}_t\hat{\mathbf{x}}_{t|t} = \mathbf{C}_t\hat{\mathbf{x}}_{t|t-1} + \left(\mathbf{C}_t \Sigma_{t|t-1} \mathbf{H}_t^\top \right) \left( \mathbf{H}_t \Sigma_{t|t-1} \mathbf{H}_t^\top  \right)^{-1} \mathbf{H}_t \mathbf{K}_t\left( \mathbf{z}_t - \mathbf{H}_t\hat{\mathbf{x}}_{t|t-1}\right)\]

\[\mathbf{C}_t\hat{\mathbf{x}}_{t|t} = \mathbf{C}_t\hat{\mathbf{x}}_{t|t-1} + \mathbf{C}_t \mathbf{K}_t\left( \mathbf{z}_t - \mathbf{H}_t\hat{\mathbf{x}}_{t|t-1}\right)\]

<p>Combining the above two results we find that</p>

\[\left(\begin{matrix} \mathbf{H}_t \\ \mathbf{C}_t\end{matrix} \right) \hat{\mathbf{x}}_{t|t} = \left(\begin{matrix} \mathbf{H}_t \\ \mathbf{C}_t\end{matrix} \right) \hat{\mathbf{x}}_{t|t-1} + \left(\begin{matrix} \mathbf{H}_t \\ \mathbf{C}_t\end{matrix} \right)  \mathbf{K}_t\left( \mathbf{z}_t - \mathbf{H}_t\hat{\mathbf{x}}_{t|t-1}\right)\]

<p>Since
\(\left(\begin{matrix} \mathbf{H}_t \\ \mathbf{C}_t\end{matrix} \right)\)
is an invertible matrix, it can be removed from both sides, and we obtain</p>

\[\boxed{ \hat{\mathbf{x}}_{t|t} = \hat{\mathbf{x}}_{t|t-1} + \mathbf{K}_t\left( \mathbf{z}_t - \mathbf{H}_t\hat{\mathbf{x}}_{t|t-1}\right)}\]

<p>Note that the covariance matrix can be obtained using the above equation as</p>

\[\boxed{\Sigma_{t|t} = \left( \mathbf{I} - \mathbf{K}_t \mathbf{H}_t\right)\Sigma_{t|t-1} \left( \mathbf{I} - \mathbf{K}_t \mathbf{H}_t\right)^\top + \mathbf{K}_t \mathbf{R}_t \mathbf{K}_t^\top}\]]]></content><author><name></name></author><summary type="html"><![CDATA[Kalman Filter Tutorial]]></summary></entry><entry><title type="html">Option Pricing</title><link href="https://azareei.github.io/blog/2021/Option-Pricing/" rel="alternate" type="text/html" title="Option Pricing" /><published>2021-02-25T08:10:00+10:00</published><updated>2021-02-25T08:10:00+10:00</updated><id>https://azareei.github.io/blog/2021/Option-Pricing</id><content type="html" xml:base="https://azareei.github.io/blog/2021/Option-Pricing/"><![CDATA[<p>I have recently got interested in options and how they are priced. In this post, I plan to briefly summarize what I have learned so far about options and how they are priced. First I start with what options are, then I cover the bounds for an option price. Next, I briefly discuss what a Weiner process or an Ito process is, and at the end I derive the famous Black-Sholes equation. Disclaimer: I don’t have any background in finance, and these are my understandings.</p>

<h1 id="options">Options</h1>

<p>An options is a contract that gives the buyer the <em>option,</em> but not obligation, to buy (i.e., <strong>Call</strong> option) or sell (i.e., <strong>Put</strong> option) an asset (such as stock) at a specific strike price before a certain date (<strong>American</strong> option) or at a certain date (<strong>European</strong> option).</p>

<p>To make it easier we focus on European options. In summary we then have the following two primary option categories: <strong>calls</strong> and <strong>puts</strong>.</p>

<ul>
  <li><strong>Call</strong>: The right to purchase stock at the strike price \(P\) from the seller of the option at time \(T.\)</li>
  <li><strong>Put</strong>: The right to sell a stock at price \(P\) to the seller of option at time \(T\).</li>
</ul>

<p>The objective of option pricing is to answer how much an option contract worth at time \(t&lt;T\) ?</p>

<div class="row mt-3" style="text-align:center;">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" width="900" src="/assets/posts/options.png" />
    </div>
</div>

<p><br /></p>

<h2 id="interest-rate">Interest rate</h2>

<p>Interest rate is the cost of borrowing money. If you borrow/lend \(X\)amount of money, you are expected to give-back/take \(Xe^{rT}\) after time \(T\). Basically it means time is money.</p>

<p><br /></p>
<h2 id="bounds-on-option-price">Bounds on Option Price</h2>

<p>We can find bounds on the values of options.</p>

<p><br /></p>
<h3 id="call-option">Call Option:</h3>

<p>The price of a call option \(c\) at the time of purchase \(t=t_0\) is  \(c = \mathbb{E}(S_T) - X\). The price of an option cannot be larger than the stock price, i.e., \(\boxed{c \leq S_0}\). If this was the case, one would sell the call options, buy the stock and invest the rest. On the other hand to find the lower bound, consider the following portfolios</p>

<ul>
  <li>One call option and \(Xe^{-rT}\) amount of money invested risk-free</li>
  <li>One stock at \(S_0\)</li>
</ul>

<p>In the first scenario the bond worth \(X\) amount of money, and the call option worth \(\max(S_T-X,0)\), so in total we will have \(X+\max(S_T-X,0) = \max(S_T,X)\) The second portfolio however worth \(S_T\). comparing the two, we find that</p>

\[\text{since }  \max(S_T,X) \geq S_T \to c + Xe^{-rT} \geq S_0 \to \boxed{c \geq S_0 - Xe^{-rT}}\]

<p><br /></p>
<h3 id="put-option">Put Option</h3>

<p>The value of a put option is \(p = X-\mathbb{E}(S_T)\). The put option can not worth more than the discounted value of the strike price, i.e., \(\boxed{p \leq Xe^{-rT}}\). Since if this wasn’t the case, one would sell the put option, invest the money, and at the exiration date, he will have more money that he needs for settlement.  To find the lower bound, we consider the following portfolios</p>

<ul>
  <li>One put option \(p\) and one stock \(S_0\)</li>
  <li>A bond paying \(X\) at time \(t=T\). It should basically cost \(Xe^{-rT}\).</li>
</ul>

<p>In the first scenario, the stock worth \(S_T\), and the put worth \(\max(X-S_T,0)\). So the portfolio, worth \(\max(X,S_T)\). on the other hand, the second portfolio worth \(X\). As a result, we have</p>

\[\text{since }\max(X,S_T) \geq X \to p + S_0 \geq Xe^{-rT} \to \boxed{p \geq Xe^{-rT}-S_0}\]

<p><br /></p>
<h3 id="put-call-parity">Put-Call Parity</h3>

\[\text{call option at }c \text{ matures to } \max(S_T-X,0) \\ \text{put option at }p \text{ matures to } \max(X-S_T,0) \\\]

<p>No, imagine we buy a call option and a bond at \(Xe^{-rT}\) on one hand, and a put option and a stock at \(S_0\). The portiolios then become</p>

<ul>
  <li>call option \(c\) + bond \(Xe^{-rT}\) matures to \(\max(S_T-X,0) + X = \max(S_T,X)\)</li>
  <li>put option \(p\) + stock \(S_0\) matures to \(\max(X-S_T,0) + S_T = \max(X,S_T)\)</li>
</ul>

<p>Since these two profiles have the same maturity, they should be equal or</p>

\[\boxed{c + Xe^{-rT} = p + S_0}\]

<p>The above equation is known as put-call parity which relates the price of a call option to the price of a call option.</p>

<p><br /></p>
<h2 id="weiner-process">Weiner Process</h2>

<p>A variable \(z\) follows a Weiner process, if the following two properties hold</p>

<ul>
  <li>The change \(\Delta z\) during period \(\Delta t\), is \(\Delta z = \epsilon \sqrt{\Delta t}\), where \(\epsilon\) has a standard normal distribution with mean zero and standard deviation unity, i.e., \(\epsilon \in \mathcal{N}(0,1)\).</li>
  <li>The values of \(\Delta z\) fr any two short intervals of time, \(\Delta t\), are independent.</li>
</ul>

<p>From the first property, we can conclude that \(\Delta z\) has a normal distribution as \(\mathcal{N}(0,\Delta t)\). If we also consider the changes from \(t=0\), to \(t=T\), we have</p>

\[z(T)-z(0) = \sum \epsilon_i \sqrt{\Delta t}\]

<p>where all of the \(\epsilon_i\)s are independent and are distributed \(\mathcal{N}(0,1)\). Since \(\epsilon_i\)s are independent, then \(z(T)-z(0) \in \mathcal(0,N\Delta t) = \mathcal{N}(0,T)\).</p>

<p><br /></p>
<h2 id="generalized-weiner-process">Generalized Weiner Process</h2>

<p>A generalized Weiner process for a random variable \(x\)  is defined as</p>

\[\Delta x = a \Delta t + b \epsilon \sqrt{\Delta t}\]

<p>where \(a,b\) are constants. If \(a\) and \(b\) are a function of \(x,t\) then, this process is called an <strong>Ito Process.</strong></p>

<p><br /></p>
<h2 id="itos-lemma">Ito’s Lemma</h2>

<p>Suppose a random variable \(x\) follows an Ito process</p>

\[dx = a(x,t) dt + b(x,t) dz\]

<p>If \(G\) is a function of \(x\) and \(t\), then</p>

\[dG = \left(\frac{\partial G}{\partial t} +  \frac{\partial G}{\partial x} a + \frac{1}{2}\frac{\partial^2 G}{\partial x^2} b^2\right) dt + \frac{\partial G}{\partial x} b ~dz\]

<p><br /></p>
<h2 id="stock-price-as-random-walk">Stock Price as Random Walk</h2>

<p>Stock price return does a random walk with drift \(\mu\) and</p>

\[\frac{dS}{S} = \mu  dt + \zeta\]

<p>where \(\zeta\) is a noise term (e.g., Gaussian) with standard deviation \(\zeta_0\) and vanishing mean. Each time step has a variance of \(\zeta_0^2\), and the number of steps are \(t/dt\). Therefore the standard deviation will be \(\zeta_0 \sqrt{t/dt}\). In order for the limit of \(dt\to0\) to make sense, \(\zeta_0 \propto \sqrt{dt}\). Therefore, we write the noise term as \(\zeta=\sigma \epsilon \sqrt{dt}\)  , where \(\sigma\) is the volatility of the stock return and \(\epsilon\) is a Gaussian random number with zero mean and unit standard deviation \(\mathcal{N}(0,1)\).</p>

<p>Let’s first take a look at the stocks random walk. In the following we assume a \(15\%\) return (\(\mu = 0.15\)) and a volatility of \(30\%\) (\(\sigma =0.15\)).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> 
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">S0</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">;</span> <span class="n">dt</span><span class="o">=</span><span class="mf">0.01</span><span class="p">;</span> <span class="n">mu</span><span class="o">=</span><span class="mf">0.15</span><span class="p">;</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.3</span><span class="p">;</span> <span class="n">T</span> <span class="o">=</span> <span class="mi">10</span><span class="p">;</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">dt</span><span class="p">)</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">S</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">S0</span><span class="p">;</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">int</span><span class="p">((</span><span class="n">T</span><span class="o">-</span><span class="n">dt</span><span class="p">)</span><span class="o">/</span><span class="n">dt</span><span class="p">)):</span>
        <span class="n">S</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">=</span><span class="n">S</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">mu</span><span class="o">*</span><span class="n">dt</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">sigma</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dt</span><span class="p">));</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="n">S</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">'b'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">yscale</span><span class="p">(</span><span class="s">'log'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'time'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Log(S/S_0)'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="row mt-3" style="text-align:center;">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" width="700" src="/assets/posts/Stocks.png" />
    </div>
</div>

<p>In summary we had</p>

\[dS = \mu S dt + \sigma S dz\]

<p>Now, imagine that \(G = \ln S\), from Ito’s Lemma, we have</p>

\[\frac{\partial G}{\partial S} = \frac{1}{S}, \quad \frac{\partial^2 G}{\partial S^2} = \frac{-1}{S^2}, \quad \frac{\partial G}{\partial t} = 0\]

<p>As a result, we find that</p>

\[dG = \left( \mu - \frac{\sigma^2}{2}\right) dt + \sigma dz\]

<p>So, we can say that</p>

\[\ln S_T - \ln S_0 \sim \mathcal{N}\left( (\mu - \frac{\sigma^2}{2})T, \sigma^2 T\right)\]

<p>Example. Imagine a stock price of \(10\$\), with annual return of \(16\%\), and volatility of \(20\%\). After \(6\) months, what is the probability distribution of stock price?</p>

\[\ln S_T - \ln10 \sim \mathcal{N}\left( (0.16-\frac{0.2^2}{2})\frac{1}{2}, 0.2^2 \frac{1}{2} \right)\\\]

\[\ln S_T - \ln10 \sim \mathcal{N}\left(0.07, 0.2 \right)\\\]

\[10 e^{0.07-0.2}\leq S_T \leq 10 e^{0.07+0.2} \to 8.78 \leq S_T \leq 13.10\]

<p>with \(90\%\) probability the stock price lies in this interval.</p>

<h1 id="derivation-of-black-scholes-merton">Derivation of Black-Scholes-Merton</h1>

<p>Assume \(f\) is the price of an option at time \(t\) contingent of stock \(S\). The variable \(f\) should be a function of \(S\) and t. Using Ito’s lemma, we have</p>

\[df = \left( \frac{\partial f}{\partial t} +  \frac{\partial f}{\partial S} \mu S + \frac{1}{2}  \frac{\partial^2 f}{\partial S^2} \sigma^2 S^2  \right) dt + \frac{\partial f}{\partial S} \sigma S dz\]

<p>Imagine a portfolio \(\Pi\) made of \(-1\) derivative and \(\partial f/\partial S\) shares, i.e.,</p>

\[\Pi = -f + \frac{\partial f}{\partial S} S\]

<p>The change in the portfolio then becomes</p>

\[\Delta \Pi = -\Delta f + \frac{\partial f}{\partial S} \Delta S\]

<p>Using the obtained results so far, we have</p>

\[\Delta \Pi = \left( -\frac{\partial f}{\partial t} - \frac{1}{2} \frac{\partial^2 f}{\partial S^2} \sigma^2 S^2 \right)\Delta t\]

<p>The above equation, does not involve \(\Delta z\), so it must be risk-less during the time \(\Delta t\). If it earned more than return, it will be a risk-less profit by borrowing money and buying the portfolio. If this portfolio earns less than r, shorting such portfolio results in a risk-less profit.</p>

\[\Delta \Pi = r \Pi \Delta t\]

\[\left( -\frac{\partial f}{\partial t} - \frac{1}{2} \frac{\partial^2 f}{\partial S^2} \sigma^2 S^2 \right)\Delta t = r \left( -f + \frac{\partial f}{\partial S} S \right) \Delta t\]

\[\boxed{\frac{\partial f}{\partial t} + \frac{1}{2}\sigma^2 S^2  \frac{\partial^2 f}{\partial S^2} +r S\frac{\partial f}{\partial S} = r f }\]

<p>The boundary condition for a European option is</p>

<ul>
  <li>Call option at \(t=T\), \(f=\max(S-X,0)\)</li>
  <li>Put option at \(t=T\), \(f = \max(X-S,T)\)</li>
</ul>

<p><br /></p>
<h2 id="solving-black-scholes-equation">Solving Black-Scholes Equation</h2>

<p>We first take the following change of variables</p>

\[\tilde S = \log S, \quad \tilde t = T-t\]

<p>As a result</p>

\[\frac{\partial f}{\partial S} = \frac{1}{S} \frac{\partial f}{\partial \tilde S} , \quad \frac{\partial^2 f}{\partial S^2} = \frac{-1}{S^2} \frac{\partial f}{\partial \tilde S}  + \frac{1}{S^2} \frac{\partial^2 f}{\partial \tilde S^2}, \quad \frac{\partial f}{\partial t} = -\frac{\partial f}{\partial t}\]

<p>w and the B-S equation becomes</p>

\[- \frac{\partial f}{\partial \tilde t} + \frac{1}{2}\sigma^2 S^2  \left( \frac{-1}{S^2} \frac{\partial f}{\partial \tilde S}  + \frac{1}{S^2} \frac{\partial^2 f}{\partial \tilde S^2} \right) +r S\left( \frac{1}{S} \frac{\partial f}{\partial \tilde S} \right) = r f\]

\[- \frac{\partial f}{\partial \tilde t} + \frac{1}{2}\sigma^2   \frac{\partial^2 f}{\partial \tilde S^2}  + \left( r- \frac{\sigma^2}{2}\right)   \frac{\partial f}{\partial \tilde S} = r f\]

<p>Next, taking out the risk-free movement in the price \(\tilde f = e^{-r\tilde t}f\), we find that</p>

\[\frac{\partial \tilde f}{\partial \tilde t} = \frac{1}{2}\sigma^2   \frac{\partial^2 \tilde  f}{\partial \tilde S^2}  + \left( r- \frac{\sigma^2}{2}\right)   \frac{\partial \tilde f}{\partial \tilde S}\]

<p>The first derivative can also be taken out using method of lines, i.e., defining a new variable \(x =\tilde S + (r-\sigma^2/2)\tilde t\), we find that</p>

\[\frac{\partial \tilde f}{\partial \tilde t} = \frac{\sigma^2}{2} \frac{\partial^2 \tilde f}{\partial x^2}\]

<p>where \(\tilde f = e^{-r\tilde t} f\), and \(x = \log S + (r-\sigma^2/2)\tilde t\). The initial condition then becoms that at \(\tilde t = 0\)</p>

<ul>
  <li>Call option \(\tilde f(x,0)=\max(e^x-X,0)\)</li>
  <li>Put option \(\tilde f(x,0)=\max(X-e^x,0)\)</li>
</ul>

<p>The Green’s function for the heats equation solution is</p>

\[\frac{\partial u}{\partial t} = D \frac{\partial^2 u}{\partial x^2} \to G(t,t',x,x') = \frac{1}{\sqrt{4\pi D |t-t'|}} e^{-\frac{(x-x')^2}{4D|t-t'|}}\]

<p>As a result the solution to the above equation becomes</p>

\[\tilde f (x,t) = \frac{1}{\sqrt{2\pi \sigma^2}} \int_{-\infty}^{\infty} e^{-\frac{(x-x')^2}{2\sigma^2 t}} \max(e^{x'}-X,0)dx'\]

\[\tilde f (x,t) = \frac{1}{\sqrt{2\pi \sigma^2}} \left[ \int_{-\infty}^{\log X} e^{-\frac{(x-x')^2}{2\sigma^2 t}} (X-e^{x'})dx' + \int_{\log X}^{\infty} e^{-\frac{(x-x')^2}{2\sigma^2 t}} (e^{x'}-X)dx'\right]\]

<p><br /></p>
<h2 id="put-call-parity-american-option">Put-Call Parity (American Option)</h2>
<p>So far, what we have discussed was about European options. But what happens in American options? Let’s start with Put-Call parity and what it looks like for American options. Consider the following portfolios</p>

<ul>
  <li>One stock \(S_0\) and a put option \(P\) at strike price \(X\)</li>
  <li>A call option \(C\) with strike price \(X\), and cash \(X\) invested at a risk free rate \(r\)</li>
</ul>

<p>Imagine that we exercise at time \(t&lt;T\). The payoff of the first portfolio is</p>

\[\left[\max(X-S_t,0) + S_t\right] e^{r(T-t)} = \max(X,S_t) e^{r(T-t)}\]

<p>where we imagined that the cash received at time \(t\), is reinvested at the risk-free rate \(r\). The payoff of the second portfolio is</p>

\[\left[ \max(S_t-X,0) + Xe^{rt}  \right] e^{r(T-t)} = \left[ \max(S_t,X) + X(e^{rt}-1)  \right] e^{r(T-t)} = \max(S_t,X)e^{r(T-t)} + X(e^{rT} - e^{r(T-t)})\]

<p>As a result we have</p>

\[S_0 + P \leq C + X\]

<p>Similarly, imagine the following portfolios</p>

<ul>
  <li>One stock \(S_0\) and a put option \(P\) at strike price \(X\)</li>
  <li>A call option \(C\) at strike price \(X\) and and \(Xe^{-rT}\) invested at a risk free rate \(r\)</li>
</ul>

<p>Again, imagine that we exercise at time \(t\), again the first portfolio payoff is</p>

\[\left[\max(X-S_t,0) + S_t\right] e^{r(T-t)} = \max(X,S_t) e^{r(T-t)}\]

<p>The second portfolio can only be traded at \(t=T\), since otherwise the money will not be enough. Therefore the payoff of the second portfolio becomes</p>

\[\max(S_T-X,0) + X = \max(S_T,X)\]

<p>As a result, the payoff of the first portfolio is larger, or</p>

\[C + Xe^{-rT} \leq S_0 + P\]

<p>In summary, we have</p>

\[\boxed{C+ Xe^{-rT} \leq S_0+P \leq C+ X}\]]]></content><author><name></name></author><summary type="html"><![CDATA[How options are priced?]]></summary></entry><entry><title type="html">You are what you seek</title><link href="https://azareei.github.io/blog/2020/Molavi/" rel="alternate" type="text/html" title="You are what you seek" /><published>2020-12-30T00:30:00+10:00</published><updated>2020-12-30T00:30:00+10:00</updated><id>https://azareei.github.io/blog/2020/Molavi</id><content type="html" xml:base="https://azareei.github.io/blog/2020/Molavi/"><![CDATA[<p><br /></p>

<p>I made the following calligraphy in persian that reads “What you seek is seeking you”. This is part of a two-couplets from Rumi</p>

<p style="text-align:right;dir:rtl;">
تا در طلب گوهر کانی کانی <br />
تا در هوس لقمهٔ نانی نانی <br />
<br /><br />
این نکتهٔ رمز اگر بدانی دانی <br />
هر چیزی که در جستن آنی آنی <br />
</p>

<p><br />
<br /></p>

<div class="row mt-3" style="text-align:center;">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" width="500" src="/assets/posts/what-you-seek.png" />
    </div>
</div>]]></content><author><name></name></author><summary type="html"><![CDATA[Molavi]]></summary></entry><entry><title type="html">Baye’s rule in odds form</title><link href="https://azareei.github.io/blog/2020/Bayes-Rule-Odds-form/" rel="alternate" type="text/html" title="Baye’s rule in odds form" /><published>2020-12-27T00:30:00+10:00</published><updated>2020-12-27T00:30:00+10:00</updated><id>https://azareei.github.io/blog/2020/Bayes-Rule-Odds-form</id><content type="html" xml:base="https://azareei.github.io/blog/2020/Bayes-Rule-Odds-form/"><![CDATA[<p>Bayes rule discusses the probability of an event based on prior knowledge and new conditions that have occurred. It basically states the following equation</p>

\[P(A\vert B) = \frac{P(A\cap B)}{P(B)} =  \frac{P(B\vert A)(A)}{P(B)} = \frac{P(B\vert A)(A)}{P(B\vert A)P(A) + P(B\vert \neg A) P(\neg A)}\]

<p>where \(A\) and \(B\) are two events and \(P(B) \neq 0\). The most commonly used form of Baye’s rule is the last equality or</p>

\[P(A\vert B) = \frac{P(B\vert A)(A)}{P(B\vert A)P(A) + P(B\vert \neg A) P(\neg A)}\]

<p>Let’s use this equation for a cancer diagnosis test. We are interested to find the probability of having cancer given a positive test result. \(A,B\) are then the events of <em>having cancer</em> and <em>testing positive</em>. The Baye’s rule then becomes</p>

\[P(C\vert +) = \frac{P(+\vert C)P(C)}{P(+\vert C)P(C) + P(+\vert \neg C) P(\neg C)}\]

<p>where \(P(C)\) is the probability of having cancer and \(P(\neg C)\)  is the probability of not having cancer. These two probabilities are priors or assumptions that we make about the initial rate of cancer in society and the test does not have to do anything with it. On the other hand, \(P(+\vert C)\) is the true positive rate of the test, and \(P(+\vert \neg C)\) is the false positive rate where they depend on the test and its performance.  All the terms (those that depend on the prior knowledge or the test performance) on the RHS are intertangled, and this makes it hard and sometimes counter-intuitive to work with Baye’s rule to understand probabilities. One easy fix is to work with odds instead of probabilities. For example, if \(P(C) = 0.1\), it means that among 10 people, the odds of having cancer to not having cancer is \(1:9\). If we use the odds, Baye’s rule becomes intuitive. We can manipulate Baye’s rule to work with the odds</p>

\[\frac{P(C\vert +)}{1-P(C\vert +)} = \frac{\frac{P(+\vert C)P(C)}{P(+\vert C)P(C) + P(+\vert \neg C) P(\neg C)}}{1- \frac{P(+\vert C)P(C)}{P(+\vert C)P(C) + P(+\vert \neg C) P(\neg C)}} = \frac{P(+\vert C)P(C)}{P(+\vert \neg C)P(\neg C)}  = \left( \frac{P(C)}{P(\neg C)}\right) \left( \frac{P(+\vert C)}{P(+\vert \neg C)} \right)\]

<p>In summary we found that</p>

\[\frac{P(C\vert +)}{P(\neg C\vert +)} = \left( \frac{P(C)}{P(\neg C)}\right) \left( \frac{P(+\vert C)}{P(+\vert \neg C)} \right)\]

<p>The left-hand side is the odds of having cancer given a positive test. On the right-hand side, the first part is the prior that doesn’t depend on the test: it’s the odds of having cancer. The second part is Baye’s factor that depends on the test: the ratio between positive results given cancer (true positive rate) and positive results without cancer (false positive rate). To make the above equation simpler, we can define the odds of an event as</p>

\[O(A_1 : A_2) = \frac{P(A_1)}{P(A_2)}\]

<p>As a result the Baye’s rule in odds form becomes</p>

\[O(C:\neg C\vert +) = O(C:\neg C) \cdot \frac{P(+\vert C)}{P(+\vert \neg C)}\]

<p>Let’s use the odds form in an example. Suppose \(1\%\) of women have breast cancer. So the odds of having breast cancer among women is \(1:9\). This is a prior assumption that we make. Now we would like to see how these odds change if one has a positive test result. Assume that we have developed a test that is \(90\%\) accurate, \(P(+\vert C) = 0.90\). This is usually called the <em>sensitivity</em> of the test, meaning that if one has breast cancer, \(90\%\) of times this test correctly gives out a positive result (true positive rate of our test). Our test is also \(80\%\) specific. This means that the true negative rate is \(P(-\vert \neg C) = 0.80\). As a result, the false-positive rate becomes \(P(+\vert \neg C) = 0.2\). Now, a woman tests positive, what are the odds of having cancer?</p>

\[O(C:\neg C\vert +) = \frac{P(C\vert +)}{P(\neg C\vert +)} = \left( \frac{0.1}{0.9} \right) \left( \frac{0.9}{0.2} \right) = \frac{1}{2}\]

<p>So initially, the odds of having cancer was \(1:9\). After the positive result, the odds are updated using Baye’s rule and it becomes \(1:2\). If the test becomes \(90\%\) specific, the above equation immediately tells us that the odds updates to \((1:9)\times(0.9/0.1) = 1:1\). This makes Baye’s rule more intuitive to work with.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Baye's rule in odds form]]></summary></entry><entry><title type="html">Sharing internet using Ethernet connection</title><link href="https://azareei.github.io/blog/2020/Sharing-internet-using-ethernet/" rel="alternate" type="text/html" title="Sharing internet using Ethernet connection" /><published>2020-12-23T08:30:00+10:00</published><updated>2020-12-23T08:30:00+10:00</updated><id>https://azareei.github.io/blog/2020/Sharing-internet-using-ethernet</id><content type="html" xml:base="https://azareei.github.io/blog/2020/Sharing-internet-using-ethernet/"><![CDATA[<p>Say you have a Jetson nano, and you want to connect it to the internet using an Ethernet connection with your PC/Laptop. This is straightforward with just a few clicks. I have Ubuntu as my OS and this is what I did</p>

<ul>
  <li>Open network manager (you can run your network manager by typing <code class="language-plaintext highlighter-rouge">nm-connection-editor</code> in your terminal)</li>
  <li>Click on creating a new connection</li>
  <li>Select Ethernet, name is as you wish, e.g., Jetson</li>
  <li>Go to “IPv4 Settings”, and select “Shared to the other computers” method</li>
  <li>Now plug in the Ethernet cable to your pc and Jetson. Select jetson connection on your PC and your PC will act as a router for your Jetson.</li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Sharing internet using Ethernet]]></summary></entry><entry><title type="html">Buffon’s Needle</title><link href="https://azareei.github.io/blog/2020/Buffons-Needle/" rel="alternate" type="text/html" title="Buffon’s Needle" /><published>2020-10-28T08:30:00+10:00</published><updated>2020-10-28T08:30:00+10:00</updated><id>https://azareei.github.io/blog/2020/Buffons-Needle</id><content type="html" xml:base="https://azareei.github.io/blog/2020/Buffons-Needle/"><![CDATA[<p>Suppose we have a floor made of parallel strips of wood, each the same width, and we drop a needle onto the floor. What is the probability that the needle lie in one of the  the two strips?</p>

<p><br /></p>
<div class="row mt-3" style="text-align:center;">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" width="300" src="/assets/posts/needle.png" />
    </div>
</div>
<p><br /></p>

<p>e.g. here in (A) the needle lies on the border line between the stripes and in (B) the needle lies in the white strip. The equation asks what is the probability of observing case (B)?</p>

<p>In order to find the probability, we need to look at the space of possible configuration for the needle and see how much of this configuration space is the part that we are interested in.</p>

<p>The center of needle has a distance \(x\) from the beginning of the strip and we assume an angle of the needle is</p>

<p><br /></p>
<div class="row mt-3" style="text-align:center;">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" width="300" src="/assets/posts/N1.png" />
    </div>
</div>
<p><br /></p>

<p>Looking at the area in the configuration space of \((x,\theta)\) we find that there will be two cases depending on if \(x\leq \ell\)  or \(x\geq \ell\). You can see the different cases in the following figure. The full configuration space is the whoe ractangle with the area of $\pi\cdot t$; and what we are interested in is the green area.</p>

<p><br /></p>
<div class="row mt-3" style="text-align:center;">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" width="600" src="/assets/posts/N2.png" />
    </div>
</div>
<p><br /></p>

<p>So the total are is \(A = \pi t\)</p>

<p>The relevant area for which the needle lies in between the stripes is</p>

\[A_g = \iint d\theta dx\]

<p>and the probability is simply \(p = A_g/A\)</p>

<p>No in order to calculate the $A_g$ we have</p>

<p>(a)  \(t\geq \ell\):</p>

<p>In this case, we have</p>

\[A_g = 4 \int_{0}^{\pi/2} d\theta~ \int_{\frac{\ell}{2}\sin\theta}^{t/2} dx = 2 \int_{0}^{\pi/2} {t} - {\ell} \sin\theta d\theta =  {\pi t} - 2{\ell}\]

<p>As a result</p>

\[p = \frac{\pi t -  2 \ell}{\pi t} = 1 - \frac{2\ell}{\pi t}\]

<p>(b)  \(t\leq \ell\):</p>

<p>We can take the integral as follows</p>

\[A_g = 4\int_0^{\sin^{-1}(t/\ell)} d\theta  \int_{\ell \sin\theta/2}^{t/2} dx =  2\int_0^{\sin^{-1}(t/\ell)} (t - \ell \sin\theta )d\theta \\\]

\[A_g = 2t\sin^{-1} \frac{t}{\ell} - 2 \ell \left[ \cos(\sin^{-1}\frac{t}{\ell}) - 1\right]\]

<p>So the probability becomes</p>

\[p = \frac{2}{\pi} \sin^{-1}\frac{t}{\ell} - \frac{2\ell}{\pi t} \left[ \cos(\sin^{-1}\frac{t}{\ell}) - 1 \right]\]

<p>So the probability of the needle lie in between the strips and not cross them is</p>

\[p = \begin{cases} \frac{2}{\pi} \sin^{-1}\frac{t}{\ell} - \frac{2\ell}{\pi t} \left[ \cos(\sin^{-1}\frac{t}{\ell}) - 1 \right] &amp; t\leq \ell \\ 1 - \frac{2\ell}{\pi t} &amp; t\geq \ell \end{cases}\]

<p>and finally this is what the probability looks like!</p>

<p><br /></p>
<div class="row mt-3" style="text-align:center;">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" width="500" src="/assets/posts/probability.png" />
    </div>
</div>]]></content><author><name></name></author><summary type="html"><![CDATA[Buffon's Needle]]></summary></entry></feed>